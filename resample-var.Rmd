---
title: "var-resample"
author: "Bj√∂rn Siepe"
date: "2022-11-10"
output: html_document
---

# Background

Goal is to compare if two VAR models $A$ and $B$ are really different from another. We use resampling to do this. \## Idea: 1. Fit $A$ and $B$ to their respective data. 2. Using parameters of $A$, generate $n$ new time series. 3. Refit $A$ and $B$ on each of the $n$ time series. 4. Obtain measure of fit, such as $MSE$. 5. Compare error distributions, either by cutoff or something like a Divergence. (6. Maybe repeat the other way around, so sampling from $B$?)

I use a developmental version of the BGGM package that returns the covariance matrix of residuals for each posterior sample. I need this to sample new data.

```{r preparations, include = FALSE}
library(tidyverse)
library(graphicalVAR)
library(doParallel)
library(doRNG)         # reproducibility for parallelization
library(mgm)
library(mlVAR)
library(BGGM)
# library("BGGM", lib.loc = "C:/Users/Bjoern/R-dev")
library(reshape2)      # Data manipulation
library(mvtnorm)       # Sim from posterior
library(stats)         # KS-Test
library(philentropy)   # divergence measures
library(todor)         # keep on track with stuff to do
source("aux_funs.R")
library(ggokabeito)
library(Matrix)
library(here)
library(rrapply)       # deeply nested list
here::i_am("var-compare.Rproj")


seed = 2022
```



Dataset overview:
- l_raw: Contains `n_mod` datasets sampled from specific DGP.
- l_res: Contains `n_ind`fitted models, one for each l_raw. Represents different people under same DGP.  
- l_params: Contains posterior beta and kappa matrices for each fitted model in l_res.
- 

New data-generating processes by Hoekstra:
She used PDC and PCC instead of beta and kappa. Maybe I won't use her dgps after all. 
For now, pretend that she used beta and kappa, so we likely use a bit sparser graphs than she did.
```{r dgp-hoekstra}
```


```{r dgp-fried}
fried_net <- readRDS(here("data/graph_fried.Rds"))

graph_fried <- list()
graph_fried$beta <- fried_net$beta
graph_fried$kappa <- fried_net$kappa


```



```{r dgp-chain graph}
# Create a chain graph, which is good to estimate with VAR models
# Not done yet

```

```{r own-graph}
graph <- readRDS(here("data/graph.RDS"))

```


Create another graph with the same structure, as "graph", but very different parameters.
```{r}


```




Now we can change all graphs according to the simulation conditions.
```{r change-graphs}

# Store graphs as list
l_graphs <- list(graph1 = graph, 
                 graph2 = graph, 
                 graph3 = graph)

# Apply change function to every graph
l_changed_graphs <- lapply(l_graphs, 
                           function(x) {change_graphs(
                                            truegraph = x, 
                                            changemax = c(1.2, 1.4, 1.6),
                                            noise = c(0.1,0.2,0.3),
                                            seed = seed)})


# Create one graph that is completely different
# diff_graph <- randomGVARmodel(Nvar = 6, probKappaEdge = 0.3, probBetaEdge = 0.5)
graph_fried$beta[1:6, 1:6]




# Check if difference is large enough
norm(graph$beta - graph_fried$beta[1:6, 1:6], type = "F")
norm(graph$beta - l_changed_graphs$graph1$noise_0.3$beta, type = "F")

# Save in list
l_changed_graphs$graph1$diffgraph <- list(
  beta = graph_fried$beta[1:6, 1:6],
  kappa = graph_fried$kappa[1:6, 1:6]
)

# Delete other graphs
l_changed_graphs$graph1[["change_1.2"]] <- NULL
l_changed_graphs$graph1[["change_1.4"]] <- NULL
l_changed_graphs$graph1[["change_1.6"]] <- NULL
l_changed_graphs$graph1[["noise_0.1"]] <- NULL


```



Then, generate rawdata from data-generating processes. We create more posterior datasets than we fit in the end, because some of them do not converge. 

Rewrite this to handle multiple data-generating processes at once: 
```{r generate-rawdata-parallel}

# Simulation conditions
n_ind <- 15 # number of individuals(so models to create) TODO change back to 100
n_tp <- c(50,100,200,400) # number of timepoints
n_postds <- 130 # number of posterior datasets
seed <- 2022

# Think about if I actually need to standardize here
# because everything will be standardized by var_estimate anyway


# Changed: instead of n_ind, we create n_postds datasets 
# because some models don't converge
before_simraw <- Sys.time()
ncores = parallel::detectCores() - 2
cl = makeCluster(ncores)
registerDoParallel(cl)
l_raw <- list()
l_raw$graph3 <- l_raw$graph2 <- l_raw$graph1 <-  list()
for(t in seq_along(n_tp)){
  l_raw$graph1[[t]] <- list()
  l_raw$graph1[[t]] <- lapply(l_changed_graphs$graph1, 
                                  function(x) {sim_raw_parallel(
                                    dgp = x, 
                                    n = n_postds, 
                                    tp = n_tp[t], 
                                    seed = seed,
                                    means = 0,
                                    standardize = TRUE)})
  
  l_raw$graph2[[t]] <- list()
  l_raw$graph2[[t]] <- lapply(l_changed_graphs$graph1, 
                                  function(x) {sim_raw_parallel(
                                    dgp = x, 
                                    n = n_postds, 
                                    tp = n_tp[t], 
                                    seed = seed,
                                    means = 0,
                                    standardize = TRUE)})
  l_raw$graph3[[t]] <- list()
  l_raw$graph3[[t]] <- lapply(l_changed_graphs$graph3, 
                                  function(x) {sim_raw_parallel(
                                    dgp = x, 
                                    n = n_postds, 
                                    tp = n_tp[t], 
                                    seed = seed,
                                    means = 0,
                                    standardize = TRUE)})
  
  
}

stopCluster(cl)

after_simraw <- Sys.time()-before_simraw  # less than 4 minutes

write_rds(l_raw, "l_raw_new.RDS")
# l_raw <- readRDS(here("l_raw.RDS"))


```



# Bayesian approach
Fit a VAR model to each raw dataset. 

Rewrite to handle all simulation conditions.
TODO: This also needs to be changed to iterate until all models converge!
And also return how often estimation failed. 

```{r}
# Bayesian model parameters
rho_sd <- 0.3      # Prior for partial correlations
beta_sd <- 1       # Prior for regression matrix
seed <- 2022
n_iter <- 5000



before_fitraw <- Sys.time()
ncores = parallel::detectCores() - 2
cl = makeCluster(ncores)
registerDoParallel(cl)
l_res <- list()
l_res$graph3 <- l_res$graph2 <- l_res$graph1 <-  list()

for(t in seq_along(n_tp)){
    l_res$graph1[[t]] <- lapply(l_raw$graph1[[t]], function(x){fit_var_parallel_merged(
    data = x, n = n_ind, nds = n_postds, rho_prior = rho_sd, beta_prior = beta_sd, seed = seed,
    iterations = n_iter, pruneresults = TRUE
  )})
    l_res$graph2[[t]] <- lapply(l_raw$graph2[[t]], function(x){fit_var_parallel_merged(
    data = x, n = n_ind, nds = n_postds, rho_prior = rho_sd, beta_prior = beta_sd, seed = seed,
    iterations = n_iter, pruneresults = TRUE
  )})
    l_res$graph3[[t]] <- lapply(l_raw$graph3[[t]], function(x){fit_var_parallel_merged(
    data = x, n = n_ind, nds = n_postds, rho_prior = rho_sd, beta_prior = beta_sd, seed = seed,
    iterations = n_iter, pruneresults = TRUE
  )}) 
  
}


stopCluster(cl)
after_fitraw <- Sys.time()-before_fitraw


write_rds(l_res, "l_res_new.Rds")
# l_res <- readRDS(here("l_res.Rds"))




```


Comparison of results with grahpicalVAR is done in another script ("bggm-var-investigation.Rmd"). Here, we look at how well our model recovered the correct parameters. 
```{r recovery}
rec_res <- list()
# Loop over timepoint conditions
for(t in 1:4){
  rec_res[[t]] <- compare_dgp(graph, est_bggm = l_res$graph1[[t]]$truegraph, comp_gvar = FALSE, est_gvar = NULL)
}
rec_res[[4]]

```



## Directly use posterior samples

Use random samples from the posterior to simulate new datasets. 


Rewrite for simulation conditions:
```{r}


before_simpost <- Sys.time()
# Setup parallel
ncores = parallel::detectCores() - 2
cl = makeCluster(ncores)
registerDoParallel(cl)
# Create storage
l_data <- list()
l_data$graph3 <- l_data$graph2 <- l_data$graph1 <- list()
# Loop over timepoint conditions
for(t in seq_along(n_tp)){
  l_data$graph1[[t]] <- lapply(l_res$graph1[[t]], function(x){sim_from_post_parallel(
    fitobj = x, n = n_ind, n_datasets = n_postds, tp = n_tp,
    iterations = n_iter, means = 0, seed = seed, convert_bggm = FALSE)})
  
  l_data$graph2[[t]] <- lapply(l_res$graph2[[t]], function(x){sim_from_post_parallel(
    fitobj = x, n = n_ind, n_datasets = n_postds, tp = n_tp,
    iterations = n_iter, means = 0, seed = seed, convert_bggm = FALSE)})

  l_data$graph3[[t]] <- lapply(l_res$graph3[[t]], function(x){sim_from_post_parallel(
    fitobj = x, n = n_ind, n_datasets = n_postds, tp = n_tp,
    iterations = n_iter, means = 0, seed = seed, convert_bggm = FALSE)})
}

stopCluster(cl)
after_simpost <- Sys.time()

write_rds(l_data, "l_data_new.Rds")
# l_data <- readRDS(here("l_data.Rds"))


```





## Norm Approach
Compute different norms between models on resampled datasets compared to reference model and then compare to actual difference between two models. 


Rewrite Code for multiple simulation conditions
```{r}
before_par_fit <- Sys.time()
ncores = parallel::detectCores() - 3
cl = makeCluster(ncores)
registerDoParallel(cl)
l_postres <- list()
l_postres$graph3 <- l_postres$graph2 <- l_postres$graph1 <- list()
for(t in seq_along(n_tp)){
  l_postres$graph1[[t]] <- lapply(l_data$graph1[[t]], function(x){fit_var_parallel_post(
                 data = x, n = n_ind, nds = n_postds,
                 rho_prior = rho_sd, beta_prior = beta_sd, seed = seed,
                 iterations = n_iter, posteriorsamples = TRUE, pruneresults = TRUE)}) 
  
  l_postres$graph2[[t]] <- lapply(l_data$graph2[[t]], function(x){fit_var_parallel_post(
                 data = x, n = n_ind, nds = n_postds,
                 rho_prior = rho_sd, beta_prior = beta_sd, seed = seed,
                 iterations = n_iter, posteriorsamples = TRUE, pruneresults = TRUE)})

  l_postres$graph3[[t]] <- lapply(l_data$graph3[[t]], function(x){fit_var_parallel_post(
                 data = x, n = n_ind, nds = n_postds,
                 rho_prior = rho_sd, beta_prior = beta_sd, seed = seed,
                 iterations = n_iter, posteriorsamples = TRUE, pruneresults = TRUE)})


}


stopCluster(cl)
after_par_fit <- Sys.time() - before_par_fit    # around 1h per graph (with 15 individuals)

write_rds(l_postres, "l_postres_new.Rds")
# l_postres <- readRDS(here("l_postres.Rds"))



```

### Evaluation

Then, start the evaluation. 


TODO: Maxdiff still throws an error here, which is indicative of non-convergence

```{r}
# All possible model combinations
comp_combinations <- expand_grid_unique(mod_a = seq(1, n_ind, 1),
                                 mod_b = seq(1, n_ind, 1))
# number of data-generating processes
n_dgp <- 1
# Number of comparison types
n_c_types <- 3

# Number of different changed graph structures
n_g_types <- 7

# Number of different tp conditions
n_t_types <- 4


dgp_names <- c("graph1", "graph2", "graph3")

change_names <- c("truegraph", "change_1.2", "change_1.4", "change_1.6",
                  "noise_0.1", "noise_0.2", "noise_0.3")


comp_names <- c("frob","maxdiff", "l1")



# Number of comparison combinations
n_c_comb <- nrow(comp_combinations)

# comp_conditions <- data.frame(
#                         postpost = rep(FALSE, n_c_comb*n_c_types*n_g_types),
#                         mod_a = rep(comp_combinations[,1], n_c_types),
#                         mod_b = rep(comp_combinations[,2], n_c_types),
#                         # fitpost_a is always the reference true graph
#                         fitpost_a = rep("l_postres$truegraph", n_c_comb*n_c_types*n_g_types),
#                         fitpost_b = rep(paste0("l_postres$", change_names), each = n_c_comb*n_c_types),
#                         # fitemp_a is always the reference true graph
#                         fitemp_a = rep("l_res$truegraph", n_c_comb*n_c_types),
#                         fitemp_b = rep("l_res", n_c_comb*n_c_types),
#                         n_datasets = rep(100, n_c_comb*n_c_types),
#                         comparison = c(rep("frob", n_c_comb), 
#                                        rep("maxdiff", n_c_comb),
#                                        rep("l1", n_c_comb)))


# Setup storage
l_comp<- list()

# Loop over data-generating processes
for(d in seq(n_dgp)){
  l_comp[[d]] <- list()
  # Loop over timepoint conditions
  for(t in seq(n_t_types)){
    l_comp[[d]][[t]] <- list() 
    # Loop over changed graphs
    for(c in seq(n_g_types)){
      l_comp[[d]][[t]][[c]] <- list()
      # Loop over model combinations
      for(m in seq(n_c_comb)){
        l_comp[[d]][[t]][[c]][[m]] <- list()
        # Loop over comparison types
        for(c_type in seq(n_c_types)){
          l_comp[[d]][[t]][[c]][[m]][[c_type]] <- list()
          l_comp[[d]][[t]][[c]][[m]][[c_type]] <- cross_compare(
                      postpost = FALSE,
                      mod_a = comp_combinations[m, 1],
                      mod_b = comp_combinations[m, 2],
                      # [[1]] indexes the truegraph
                      fitpost_a = l_postres[[d]][[t]][[1]],
                      fitpost_b = l_postres[[d]][[t]][[c]],
                      fitemp_a = l_res[[d]][[t]][[1]],
                      fitemp_b = l_res[[d]][[t]][[c]],
                      n_datasets = 100,
                      comparison = comp_names[c_type])
          # Store relevant parameters for evaluation
         l_comp[[d]][[t]][[c]][[m]][[c_type]]$params <- data.frame(
           dgp = dgp_names[d],
           tp = n_tp[t],
           comp_graph = change_names[c]
         )
        } 

        
        
      }
    }
  }
}



```




Evaluation: 
- number of posterior sample distances > empirical distance

```{r eval_comp}
l_eval <- list()
# Loop over data-generating processes
for(d in seq(n_dgp)){
  l_eval[[d]] <- list()
  # Loop over timepoint conditions
  for(t in seq(n_t_types)){
    l_eval[[d]][[t]] <- list() 
    # Loop over changed graphs
    for(c in seq(n_g_types)){
      l_eval[[d]][[t]][[c]] <- list()
      # Loop over model combinations
      for(m in seq(n_c_comb)){
        l_eval[[d]][[t]][[c]][[m]] <- list()
        l_eval[[d]][[t]][[c]][[m]] <- lapply(l_comp[[d]][[t]][[c]][[m]],cross_compare_eval)

      }
    }
  }
}


# Now unpack
df_eval <- rrapply::rrapply(l_eval, how = "bind")




```


Now we can plot the results. First, power to detect true differences:
```{r}
eval_plot_diffgraph <- df_eval %>% 
  pivot_longer(cols = c("beta_a", "beta_b", "pcor_a", "pcor_b"), names_to = "res") %>% 
  # mutate(across(c(comp, dgp, tp, comp_graph, res),
                # ~ as.factor(.))) %>% 
  filter(dgp == "graph1") %>% 
  # comp_graph == truegraph is comparison with same dgp, so null
  filter(comp_graph != "truegraph") %>% 
  mutate(sig05 = ifelse(value <= 5, 1, 0),
         sig01 = ifelse(value <= 1, 1, 0)) %>%
  group_by(comp, tp, comp_graph, res) %>%
  mutate(n_comp = n()) %>%  
  summarize(power05 = sum(sig05)/n_comp) %>% 
  # somehow does not cut everything if I don't do this
  summarize(power05 = mean(power05)) %>% 
  ggplot(aes(x = tp, y = power05, col = comp)) + 
  geom_line()+
  facet_grid(comp_graph ~ res)+
  theme_minimal()+
  scale_color_okabe_ito()+
  labs(title = "Graph Fried (2020) as Comparison",
       caption = "Rho_SD = 0.3, Beta_sd = 1")
ggsave("eval_plot_diffgraph.svg", device = "svg", path = here("figures/"))  


```







## Compare with full posterior differences
Instead of only taking the empirical difference between fits, we generate a distribution of posterior differences _between_ models.  

```{r posterior-differences-between}
post_diff <- cross_compare_post(comparison = "l1")
post_diff %>% 
  bind_rows(., .id = "matrix") %>% 
  pivot_longer(cols = c("null", "emp"), names_to = "dist") %>% 
  ggplot(aes(x = value, group = dist, color = dist, fill = dist))+
  geom_density()+
  theme_minimal()
```



## Compare with bootstrapped posterior models
We now generate a distribution of posterior differences _within_ models. 
```{r posterior differences-within}
wc_res <- within_compare(
                      mod_a = 1,
                      mod_b = 2,
                      # [[1]] indexes the truegraph
                      fitpost_a = l_postres[[1]][[3]][[1]],
                      fitpost_b = l_postres[[1]][[3]][[2]],
                      fitemp_a = l_res[[1]][[3]][[1]],
                      fitemp_b = l_res[[1]][[3]][[2]],
                      n_datasets = 100,
                      comparison = "frob",
                      n_draws = 1000)


wc_res %>% 
  filter(mat == "beta") %>% 
  # pivot_longer(cols = c("null", "emp"), names_to = "res") %>% 
  ggplot(aes(x = null))+
  geom_density()+
  geom_vline(aes(xintercept = emp))


```

Do this for all models.
Still throws an error sometimes
```{r}
# All possible model combinations
comp_combinations <- expand_grid_unique(mod_a = seq(1, n_ind, 1),
                                 mod_b = seq(1, n_ind, 1))
# number of data-generating processes
n_dgp <- 3
# Number of comparison types
n_c_types <- 3

# Number of different changed graph structures
n_g_types <- 7

# Number of different tp conditions
n_t_types <- 4

dgp_names <- c("graph1", "graph2", "graph3")

change_names <- c("truegraph", "change_1.2", "change_1.5", "change_1.7",
                  "noise_0.1", "noise_0.2", "noise_0.3")


comp_names <- c("frob","maxdiff", "l1")



# Number of comparison combinations
n_c_comb <- nrow(comp_combinations)

# Setup storage
l_comp_within<- list()

# Loop over data-generating processes
for(d in seq(n_dgp)){
  l_comp_within[[d]] <- list()
  # Loop over timepoint conditions
  for(t in seq(n_t_types)){
    l_comp_within[[d]][[t]] <- list() 
    # Loop over changed graphs
    for(c in seq(n_g_types)){
      l_comp_within[[d]][[t]][[c]] <- list()
      # Loop over model combinations
      for(m in seq(n_c_comb)){
        l_comp_within[[d]][[t]][[c]][[m]] <- list()
        # Loop over comparison types
        for(c_type in seq(n_c_types)){
          l_comp_within[[d]][[t]][[c]][[m]][[c_type]] <- list()
          l_comp_within[[d]][[t]][[c]][[m]][[c_type]]$res <- within_compare(
                      mod_a = comp_combinations[m, 1],
                      mod_b = comp_combinations[m, 2],
                      # [[1]] indexes the truegraph
                      fitpost_a = l_postres[[d]][[t]][[1]],
                      fitpost_b = l_postres[[d]][[t]][[c]],
                      fitemp_a = l_res[[d]][[t]][[1]],
                      fitemp_b = l_res[[d]][[t]][[c]],
                      n_datasets = 100,
                      n_draws = 1000,
                      comparison = comp_names[c_type])
         # Store relevant parameters for evaluation
         l_comp_within[[d]][[t]][[c]][[m]][[c_type]]$params <- list(
           dgp = dgp_names[d],
           tp = n_tp[t],
           comp_graph = change_names[c]
         )
        } 

        
        
      }
    }
  }
}


```

Now evaluate this: (does not work properly yet)
```{r}
within_compare_eval <- function(l_res,
                                pcor = TRUE){
  ### Betas
  df_res <- as.data.frame(l_res$res)
  df_res_beta <- subset(df_res, mat == "beta")
  
  # Obtain model indexes
  model_ind_a <- unique(df_res_beta$model_ind)[1]
  model_ind_b <- unique(df_res_beta$model_ind)[2]
  
  # Number of posterior difference > empirical difference
  teststat_a_beta <- sum(df_res_beta$null[df_res_beta$model_ind == model_ind_a] > df_res_beta$emp[df_res_beta$model_ind == model_ind_a], na.rm = TRUE)
  teststat_b_beta <- sum(df_res_beta$null[df_res_beta$model_ind == model_ind_b] > df_res_beta$emp[df_res_beta$model_ind == model_ind_b], na.rm = TRUE)
  
    
    
  if(isTRUE(pcor)){
    ### Pcor
    df_res_pcor <- subset(df_res, mat == "pcor")
    # Obtain model indexes
    model_ind_a <- unique(df_res_pcor$model_ind)[1]
    model_ind_b <- unique(df_res_pcor$model_ind)[2]
    
    # Number of posterior difference > empirical difference
    teststat_a_pcor <- sum(df_res_pcor$null[df_res_pcor$model_ind == model_ind_a] > df_res_pcor$emp[df_res_pcor$model_ind == model_ind_a], na.rm = TRUE)
    teststat_b_pcor <- sum(df_res_pcor$null[df_res_pcor$model_ind == model_ind_b] > df_res_pcor$emp[df_res_pcor$model_ind == model_ind_b], na.rm = TRUE)
    
  }

  testres <- list(beta_a = teststat_a_beta,
                  beta_b = teststat_b_beta,
                  pcor_a = teststat_a_pcor,
                  pcor_b = teststat_b_pcor,
                  comp = df_res_beta$comp[[1]], # get type of comparison
                  dgp = l_res$params$dgp,
                  tp = l_res$params$tp,
                  comp_graph = l_res$params$comp_graph)  
  testres
}



within_compare_eval(l_res = l_comp_within[[1]][[3]][[4]][[1]][[1]])



# Set up for all simulation conditions
l_eval_within <- list()
# Loop over data-generating processes
# TODO change back to seg(n_dgp)
for(d in seq(n_dgp)){
  l_eval_within[[d]] <- list()
  # Loop over timepoint conditions
  for(t in seq(n_t_types)){
    l_eval_within[[d]][[t]] <- list() 
    # Loop over changed graphs
    for(c in seq(n_g_types)){
      l_eval_within[[d]][[t]][[c]] <- list()
      # Loop over model combinations
      for(m in seq(n_c_comb)){
        l_eval_within[[d]][[t]][[c]][[m]] <- list()
        l_eval_within[[d]][[t]][[c]][[m]] <- lapply(l_comp_within[[d]][[t]][[c]][[m]],within_compare_eval)

      }
    }
  }
}


# Now unpack
df_eval_within <- rrapply::rrapply(l_eval_within, how = "bind")


```


Plot
```{r}
df_eval_within %>% 
  pivot_longer(cols = c("beta_a", "beta_b", "pcor_a", "pcor_b"), names_to = "res") %>% 
  # mutate(across(c(comp, dgp, tp, comp_graph, res),
                # ~ as.factor(.))) %>% 
  filter(dgp == "graph1") %>% 
  # comp_graph == truegraph is comparison with same dgp, so null
  filter(comp_graph != "truegraph") %>% 
  mutate(sig05 = ifelse(value < 50, 1, 0),
         sig01 = ifelse(value < 10, 1, 0)) %>%
  group_by(comp, tp, comp_graph, res) %>%
  mutate(n_comp = n()) %>%  
  summarize(power05 = sum(sig05)/n_comp) %>% 
  # somehow does not cut everything if I don't do this
  summarize(power05 = mean(power05)) %>% 
  ggplot(aes(x = tp, y = power05, col = comp)) + 
  geom_line()+
  facet_grid(comp_graph ~ res)+
  theme_minimal()+
  scale_color_okabe_ito()



```



# New Approach (25.01.2023): Posterior Comparison without Predictive
The idea is to compare distances between matrices in the posterior as a reference distribution, without having to compute any posterior predictives.


```{r}
wc2 <- list()
wc2$res <- within_compare(
                      mod_a = 1,
                      mod_b = 2,
                      # [[]][[]][[1]] indexes the truegraph
                      fitpost_a = l_res[[1]][[3]][[1]],
                      fitpost_b = l_res[[1]][[3]][[4]],
                      fitemp_a = l_res[[1]][[3]][[1]],
                      fitemp_b = l_res[[1]][[3]][[4]],
                      n_datasets = 100,
                      comparison = "frob",
                      n_draws = 1000,
                      postpred = FALSE)



```


Compare all models:
```{r}
within_compare_eval(wc2)
# Setup storage
l_comp_within<- list()

# Loop over data-generating processes
for(d in seq(n_dgp)){
  l_comp_within[[d]] <- list()
  # Loop over timepoint conditions
  for(t in seq(n_t_types)){
    l_comp_within[[d]][[t]] <- list() 
    # Loop over changed graphs
    for(c in seq(n_g_types)){
      l_comp_within[[d]][[t]][[c]] <- list()
      # Loop over model combinations
      for(m in seq(n_c_comb)){
        l_comp_within[[d]][[t]][[c]][[m]] <- list()
        # Loop over comparison types
        for(c_type in seq(n_c_types)){
          l_comp_within[[d]][[t]][[c]][[m]][[c_type]] <- list()
          l_comp_within[[d]][[t]][[c]][[m]][[c_type]]$res <- within_compare(
                      mod_a = comp_combinations[m, 1],
                      mod_b = comp_combinations[m, 2],
                      # [[1]] indexes the truegraph
                      fitpost_a = l_res[[d]][[t]][[1]],
                      fitpost_b = l_res[[d]][[t]][[c]],
                      fitemp_a = l_res[[d]][[t]][[1]],
                      fitemp_b = l_res[[d]][[t]][[c]],
                      n_datasets = 100,
                      n_draws = 1000,
                      comparison = comp_names[c_type],
                      postpred = FALSE)
         # Store relevant parameters for evaluation
         l_comp_within[[d]][[t]][[c]][[m]][[c_type]]$params <- list(
           dgp = dgp_names[d],
           tp = n_tp[t],
           comp_graph = change_names[c]
         )
        } 

        
        
      }
    }
  }
}

```

Evaluate all models:
```{r}
# Set up for all simulation conditions
l_eval_within <- list()
# Loop over data-generating processes
# TODO change back to seg(n_dgp)
for(d in seq(n_dgp)){
  l_eval_within[[d]] <- list()
  # Loop over timepoint conditions
  for(t in seq(n_t_types)){
    l_eval_within[[d]][[t]] <- list() 
    # Loop over changed graphs
    for(c in seq(n_g_types)){
      l_eval_within[[d]][[t]][[c]] <- list()
      # Loop over model combinations
      for(m in seq(n_c_comb)){
        l_eval_within[[d]][[t]][[c]][[m]] <- list()
        l_eval_within[[d]][[t]][[c]][[m]] <- lapply(l_comp_within[[d]][[t]][[c]][[m]],within_compare_eval)

      }
    }
  }
}


# Now unpack
df_eval_within <- rrapply::rrapply(l_eval_within, how = "bind")



```


Plot results
```{r}
plot_within_emppost <- df_eval_within %>% 
  pivot_longer(cols = c("beta_a", "beta_b", "pcor_a", "pcor_b"), names_to = "res") %>% 
  # mutate(across(c(comp, dgp, tp, comp_graph, res),
                # ~ as.factor(.))) %>% 
  filter(dgp == "graph1") %>% 
  # comp_graph == truegraph is comparison with same dgp, so null
  # filter(comp_graph != "truegraph") %>% 
  mutate(sig05 = ifelse(value < 50, 1, 0),
         sig01 = ifelse(value < 10, 1, 0)) %>%
  group_by(comp, tp, comp_graph, res) %>%
  mutate(n_comp = n()) %>%  
  summarize(power05 = sum(sig05)/n_comp) %>% 
  # somehow does not cut everything if I don't do this
  summarize(power05 = mean(power05)) %>% 
  ggplot(aes(x = tp, y = power05, col = comp)) + 
  geom_line()+
  facet_grid(comp_graph ~ res)+
  theme_minimal()+
  scale_color_okabe_ito()

ggsave("plot_within_emppost.svg", plot_within_emppost, device = "svg", path = here("figures/"))  


```

