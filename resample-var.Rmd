---
title: "var-resample"
author: "Bj√∂rn Siepe"
date: "2022-11-10"
output: html_document
---

# Background

Goal is to compare if two VAR models $A$ and $B$ are really different from another. We use resampling to do this. \## Idea: 1. Fit $A$ and $B$ to their respective data. 2. Using parameters of $A$, generate $n$ new time series. 3. Refit $A$ and $B$ on each of the $n$ time series. 4. Obtain measure of fit, such as $MSE$. 5. Compare error distributions, either by cutoff or something like a Divergence. (6. Maybe repeat the other way around, so sampling from $B$?)

I use a developmental version of the BGGM package that returns the covariance matrix of residuals for each posterior sample. I need this to sample new data.

```{r preparations, include = FALSE}
library(tidyverse)
library(graphicalVAR)
library(doParallel)
library(doRNG)         # reproducibility for parallelization
library(mgm)
library(mlVAR)
library(BGGM)
# library("BGGM", lib.loc = "C:/Users/Bjoern/R-dev")
library(reshape2)      # Data manipulation
library(mvtnorm)       # Sim from posterior
library(stats)         # KS-Test
library(philentropy)   # divergence measures
library(todor)         # keep on track with stuff to do
source("aux_funs.R")
library(ggokabeito)
library(Matrix)
library(here)
library(rrapply)       # deeply nested list
here::i_am("var-compare.Rproj")


seed = 2022
```



Dataset overview:
- l_raw: Contains `n_mod` datasets sampled from specific DGP.
- l_res: Contains `n_ind`fitted models, one for each l_raw. Represents different people under same DGP.  
- l_params: Contains posterior beta and kappa matrices for each fitted model in l_res.
- 

New data-generating processes by Hoekstra:
She used PDC and PCC instead of beta and kappa. Maybe I won't use her dgps after all. 
For now, pretend that she used beta and kappa, so we likely use a bit sparser graphs than she did.
```{r dgp-hoekstra}
```


```{r dgp-fried}
fried_net <- readRDS(here("data/graph_fried.Rds"))

graph_fried <- list()
graph_fried$beta <- fried_net$beta
graph_fried$kappa <- fried_net$kappa





```



```{r dgp-chain graph}
# Create a chain graph, which is good to estimate with VAR models
# Not done yet





```

```{r own-graph}
graph <- readRDS(here("data/graph.RDS"))

# delete PCC (not needed)
graph <- graph[-3]

```


Create another graph with the same structure, as "graph", but very different parameters.
```{r}


```

Graph with non-sparsity
```{r}
graph_full_f <- readRDS("full_graph.RDS")

graph_full <- list(
  beta = graph_full_f$beta_mu,
  kappa = graph_full_f$kappa_mu
)
# Non-symmetric Kappa due to rounding errors
graph_full$kappa <- as.matrix(Matrix::forceSymmetric(graph_full$kappa))



```


## Change Graphs
Now we can change all graphs according to the simulation conditions.
```{r change-graphs}

# Store graphs as list
l_graphs <- list(graph1 = graph,
                 graph2 = graph_full, 
                 graph3 = graph_fried)

# Apply change function to every graph
# TODO needs to check for positive semidefiniteness somehow! does not work for graph (with small kappa varianceds)
l_changed_graphs <- lapply(l_graphs, 
                           function(x) {change_graphs(
                                            truegraph = x, 
                                            changemax = c(1.2, 1.4, 1.6),
                                            noise = c(0.1,0.2,0.3),
                                            seed = seed)} )

change_graphs(truegraph = graph, changemax = c(1.2, 1.4, 1.6), noise = c(0.1, 0.2, 0.3), seed = seed)

# Create one graph that is completely different
# diff_graph <- randomGVARmodel(Nvar = 6, probKappaEdge = 0.3, probBetaEdge = 0.5)
graph_fried$beta[1:6, 1:6]




# Check if difference is large enough
norm(graph$beta - graph_fried$beta[1:6, 1:6], type = "F")
norm(graph$beta - l_changed_graphs$graph1$noise_0.3$beta, type = "F")

# # Save in list
# l_changed_graphs$graph1$diffgraph <- list(
#   beta = graph_fried$beta[1:6, 1:6],
#   kappa = graph_fried$kappa[1:6, 1:6]
# )

# # Delete other graphs
# l_changed_graphs$graph1[["change_1.2"]] <- NULL
# l_changed_graphs$graph1[["change_1.4"]] <- NULL
# l_changed_graphs$graph1[["change_1.6"]] <- NULL
# l_changed_graphs$graph1[["noise_0.1"]] <- NULL
# write_rds(l_changed_graphs, "l_changed_graphs.RDS")

```







## Simulate Raw Data
Then, generate rawdata from data-generating processes. We create more posterior datasets than we fit in the end, because some of them may not converge when using the posterior predictive approach. 

```{r generate-rawdata-parallel}

# Simulation conditions
n_ind <- 50 # number of individuals(so models to create) 
n_tp <- c(50,100,200,400) # number of timepoints
n_postds <- 130 # number of posterior datasets
seed <- 2022

# Bayesian model parameters
rho_sd <- 0.5      # Prior for partial correlations
beta_sd <- 1       # Prior for regression matrix
seed <- 2022
n_iter <- 5000


# Think about if I actually need to standardize here
# because everything will be standardized by var_estimate anyway


# Changed: instead of n_ind, we create n_postds datasets 
# because some models don't converge
before_simraw <- Sys.time()
ncores = parallel::detectCores() - 4
cl = makeCluster(ncores)
registerDoParallel(cl)
l_raw <- list()
l_raw$graph3 <- l_raw$graph2 <- l_raw$graph1 <-  list()
for(t in seq_along(n_tp)){
  l_raw$graph1[[t]] <- list()
  l_raw$graph1[[t]] <- lapply(l_changed_graphs$graph1, 
                                  function(x) {sim_raw_parallel(
                                    dgp = x, 
                                    n = n_postds, 
                                    tp = n_tp[t], 
                                    seed = seed,
                                    means = 0,
                                    standardize = TRUE)})
  
  l_raw$graph2[[t]] <- list()
  l_raw$graph2[[t]] <- lapply(l_changed_graphs$graph1, 
                                  function(x) {sim_raw_parallel(
                                    dgp = x, 
                                    n = n_postds, 
                                    tp = n_tp[t], 
                                    seed = seed,
                                    means = 0,
                                    standardize = TRUE)})
  l_raw$graph3[[t]] <- list()
  l_raw$graph3[[t]] <- lapply(l_changed_graphs$graph3, 
                                  function(x) {sim_raw_parallel(
                                    dgp = x, 
                                    n = n_postds, 
                                    tp = n_tp[t], 
                                    seed = seed,
                                    means = 0,
                                    standardize = TRUE)})
  
  
}

stopCluster(cl)

after_simraw <- Sys.time()-before_simraw  # less than 4 minutes

# write_rds(l_raw, "l_raw_new.RDS")
# l_raw <- readRDS(here("l_raw_new.RDS"))


```



# Bayesian approach
Fit a VAR model to each raw dataset. 

```{r}
# for testing
n_ind = 5
nds = 5

before_fitraw <- Sys.time()
ncores = parallel::detectCores() - 2
cl = makeCluster(ncores)
registerDoParallel(cl)
l_res <- list()
l_res$graph3 <- l_res$graph2 <- l_res$graph1 <-  list()

# for(t in seq_along(n_tp)){
#     l_res$graph1[[t]] <- lapply(l_raw$graph1[[t]], function(x){fit_var_parallel_merged(
#     data = x, n = n_ind, nds = n_postds, rho_prior = rho_sd, beta_prior = beta_sd, seed = seed,
#     iterations = n_iter, pruneresults = TRUE, dgp_name = "graph1", save_files = TRUE
#   )})
#     l_res$graph2[[t]] <- lapply(l_raw$graph2[[t]], function(x){fit_var_parallel_merged(
#     data = x, n = n_ind, nds = n_postds, rho_prior = rho_sd, beta_prior = beta_sd, seed = seed,
#     iterations = n_iter, pruneresults = TRUE, dgp_name = "graph2", save_files = TRUE
#   )})
#     l_res$graph3[[t]] <- lapply(l_raw$graph3[[t]], function(x){fit_var_parallel_merged(
#     data = x, n = n_ind, nds = n_postds, rho_prior = rho_sd, beta_prior = beta_sd, seed = seed,
#     iterations = n_iter, pruneresults = TRUE, dgp_name = "graph3", save_files = TRUE
#   )}) 
#   
# }

for(t in seq_along(n_tp)){
    lapply(l_raw$graph1[[t]], function(x){fit_var_parallel_merged(
    data = x, n = n_ind, nds = n_postds, rho_prior = rho_sd, beta_prior = beta_sd, seed = seed,
    iterations = n_iter, pruneresults = TRUE, dgp_name = "graph1", save_files = TRUE
  )})
    lapply(l_raw$graph2[[t]], function(x){fit_var_parallel_merged(
    data = x, n = n_ind, nds = n_postds, rho_prior = rho_sd, beta_prior = beta_sd, seed = seed,
    iterations = n_iter, pruneresults = TRUE, dgp_name = "graph2", save_files = TRUE
  )})
    lapply(l_raw$graph3[[t]], function(x){fit_var_parallel_merged(
    data = x, n = n_ind, nds = n_postds, rho_prior = rho_sd, beta_prior = beta_sd, seed = seed,
    iterations = n_iter, pruneresults = TRUE, dgp_name = "graph3", save_files = TRUE
  )})

}
stopCluster(cl)
after_fitraw <- Sys.time()-before_fitraw


# write_rds(l_res, "l_res_new.Rds")
# l_res <- readRDS(here("l_res_new.Rds"))




```


Comparison of results with grahpicalVAR is done in another script ("bggm-var-investigation.Rmd"). Here, we can look at how well our model recovered the correct parameters. 
```{r recovery}
rec_res <- list()
# Loop over timepoint conditions
for(t in 1:4){
  rec_res[[t]] <- compare_dgp(graph, est_bggm = l_res$graph1[[t]]$truegraph, comp_gvar = FALSE, est_gvar = NULL)
}
rec_res[[4]]
median(as.matrix(abs(rec_res[[4]]$diff_beta_mean$true_bggm)))

```



## Directly use posterior samples

Use random samples from the posterior to simulate new datasets. 


Rewrite for simulation conditions:
```{r}
before_simpost <- Sys.time()
# Setup parallel
ncores = parallel::detectCores() - 2
cl = makeCluster(ncores)
registerDoParallel(cl)
# Create storage
l_data <- list()
l_data$graph3 <- l_data$graph2 <- l_data$graph1 <- list()
# Loop over timepoint conditions
for(t in seq_along(n_tp)){
  l_data$graph1[[t]] <- lapply(l_res$graph1[[t]], function(x){sim_from_post_parallel(
    fitobj = x, n = n_ind, n_datasets = n_postds, tp = n_tp,
    iterations = n_iter, means = 0, seed = seed, convert_bggm = FALSE)})
  
  l_data$graph2[[t]] <- lapply(l_res$graph2[[t]], function(x){sim_from_post_parallel(
    fitobj = x, n = n_ind, n_datasets = n_postds, tp = n_tp,
    iterations = n_iter, means = 0, seed = seed, convert_bggm = FALSE)})

  l_data$graph3[[t]] <- lapply(l_res$graph3[[t]], function(x){sim_from_post_parallel(
    fitobj = x, n = n_ind, n_datasets = n_postds, tp = n_tp,
    iterations = n_iter, means = 0, seed = seed, convert_bggm = FALSE)})
}

stopCluster(cl)
after_simpost <- Sys.time()

write_rds(l_data, "l_data_new.Rds")
# l_data <- readRDS(here("l_data.Rds"))


```

Double check simulation: 
```{r}
sim_check <- graphicalVARsim(3000, beta = t(l_res$graph1[[4]]$truegraph[[1]]$beta_mu), kappa = l_res$graph1[[4]]$truegraph[[1]]$kappa_mu)
sim_check <- as.data.frame(sim_check)
sim_check_res <- var_estimate(sim_check)

sim_check_res$beta_mu-l_res$graph1[[4]]$truegraph[[1]]$beta_mu

```








# Posterior Predictive Simulation Approach
Compute different norms between models on resampled datasets compared to reference model and then compare to actual difference between two models. 


```{r}
before_par_fit <- Sys.time()
ncores = parallel::detectCores() - 3
cl = makeCluster(ncores)
registerDoParallel(cl)
l_postres <- list()
l_postres$graph3 <- l_postres$graph2 <- l_postres$graph1 <- list()
for(t in seq_along(n_tp)){
  l_postres$graph1[[t]] <- lapply(l_data$graph1[[t]], function(x){fit_var_parallel_post(
                 data = x, n = n_ind, nds = n_postds,
                 rho_prior = rho_sd, beta_prior = beta_sd, seed = seed,
                 iterations = n_iter, posteriorsamples = TRUE, pruneresults = TRUE)}) 
  
  l_postres$graph2[[t]] <- lapply(l_data$graph2[[t]], function(x){fit_var_parallel_post(
                 data = x, n = n_ind, nds = n_postds,
                 rho_prior = rho_sd, beta_prior = beta_sd, seed = seed,
                 iterations = n_iter, posteriorsamples = TRUE, pruneresults = TRUE)})

  l_postres$graph3[[t]] <- lapply(l_data$graph3[[t]], function(x){fit_var_parallel_post(
                 data = x, n = n_ind, nds = n_postds,
                 rho_prior = rho_sd, beta_prior = beta_sd, seed = seed,
                 iterations = n_iter, posteriorsamples = TRUE, pruneresults = TRUE)})


}


stopCluster(cl)
after_par_fit <- Sys.time() - before_par_fit    # around 1h per graph (with 15 individuals)

write_rds(l_postres, "l_postres_new.Rds")
# l_postres <- readRDS(here("l_postres.Rds"))



```

### Evaluation

Then, start the evaluation. 
Maxdiff still throws an error here, which is indicative of non-convergence

```{r}
# All possible model combinations
comp_combinations <- expand_grid_unique(mod_a = seq(1, n_ind, 1),
                                 mod_b = seq(1, n_ind, 1))
# number of data-generating processes
n_dgp <- 1
# Number of comparison types
n_c_types <- 3

# Number of different changed graph structures
n_g_types <- 7

# Number of different tp conditions
n_t_types <- 4


dgp_names <- c("graph1", "graph2", "graph3")
change_names <- c("truegraph", "change_1.2", "change_1.4", "change_1.6",
                  "noise_0.1", "noise_0.2", "noise_0.3")
comp_names <- c("frob","maxdiff", "l1")


# Number of comparison combinations
n_c_comb <- nrow(comp_combinations)


# Setup storage
l_comp<- list()

# Loop over data-generating processes
for(d in seq(n_dgp)){
  l_comp[[d]] <- list()
  # Loop over timepoint conditions
  for(t in seq(n_t_types)){
    l_comp[[d]][[t]] <- list() 
    # Loop over changed graphs
    for(c in seq(n_g_types)){
      l_comp[[d]][[t]][[c]] <- list()
      # Loop over model combinations
      for(m in seq(n_c_comb)){
        l_comp[[d]][[t]][[c]][[m]] <- list()
        # Loop over comparison types
        for(c_type in seq(n_c_types)){
          l_comp[[d]][[t]][[c]][[m]][[c_type]] <- list()
          l_comp[[d]][[t]][[c]][[m]][[c_type]] <- cross_compare(
                      postpost = FALSE,
                      mod_a = comp_combinations[m, 1],
                      mod_b = comp_combinations[m, 2],
                      # [[1]] indexes the truegraph
                      fitpost_a = l_postres[[d]][[t]][[1]],
                      fitpost_b = l_postres[[d]][[t]][[c]],
                      fitemp_a = l_res[[d]][[t]][[1]],
                      fitemp_b = l_res[[d]][[t]][[c]],
                      n_datasets = 100,
                      comparison = comp_names[c_type])
          # Store relevant parameters for evaluation
         l_comp[[d]][[t]][[c]][[m]][[c_type]]$params <- data.frame(
           dgp = dgp_names[d],
           tp = n_tp[t],
           comp_graph = change_names[c]
         )
        } 

        
        
      }
    }
  }
}



```




Evaluation: 
- number of posterior sample distances > empirical distance

```{r eval_comp}
l_eval <- list()
# Loop over data-generating processes
for(d in seq(n_dgp)){
  l_eval[[d]] <- list()
  # Loop over timepoint conditions
  for(t in seq(n_t_types)){
    l_eval[[d]][[t]] <- list() 
    # Loop over changed graphs
    for(c in seq(n_g_types)){
      l_eval[[d]][[t]][[c]] <- list()
      # Loop over model combinations
      for(m in seq(n_c_comb)){
        l_eval[[d]][[t]][[c]][[m]] <- list()
        l_eval[[d]][[t]][[c]][[m]] <- lapply(l_comp[[d]][[t]][[c]][[m]],cross_compare_eval)

      }
    }
  }
}


# Now unpack
df_eval <- rrapply::rrapply(l_eval, how = "bind")




```


Now we can plot the results. First, power to detect true differences:
```{r}
eval_plot_diffgraph <- df_eval %>% 
  pivot_longer(cols = c("beta_a", "beta_b", "pcor_a", "pcor_b"), names_to = "res") %>% 
  # mutate(across(c(comp, dgp, tp, comp_graph, res),
                # ~ as.factor(.))) %>% 
  filter(dgp == "graph1") %>% 
  # comp_graph == truegraph is comparison with same dgp, so null
  filter(comp_graph != "truegraph") %>% 
  mutate(sig05 = ifelse(value <= 5, 1, 0),
         sig01 = ifelse(value <= 1, 1, 0)) %>%
  group_by(comp, tp, comp_graph, res) %>%
  mutate(n_comp = n()) %>%  
  summarize(power05 = sum(sig05)/n_comp) %>% 
  # somehow does not cut everything if I don't do this
  summarize(power05 = mean(power05)) %>% 
  ggplot(aes(x = tp, y = power05, col = comp)) + 
  geom_line()+
  facet_grid(comp_graph ~ res)+
  theme_minimal()+
  scale_color_okabe_ito()+
  labs(title = "Graph Fried (2020) as Comparison",
       caption = "Rho_SD = 0.3, Beta_sd = 1")
ggsave("eval_plot_diffgraph.svg", device = "svg", path = here("figures/"))  


```







## Compare with full posterior differences
Instead of only taking the empirical difference between fits, we generate a distribution of posterior differences _between_ models.  

```{r posterior-differences-between}
post_diff <- cross_compare_post(comparison = "l1")
post_diff %>% 
  bind_rows(., .id = "matrix") %>% 
  pivot_longer(cols = c("null", "emp"), names_to = "dist") %>% 
  ggplot(aes(x = value, group = dist, color = dist, fill = dist))+
  geom_density()+
  theme_minimal()
```



## Compare with bootstrapped posterior models
We now generate a distribution of posterior differences _within_ models. 
```{r posterior differences-within}
wc_res <- within_compare(
                      mod_a = 1,
                      mod_b = 2,
                      # [[1]] indexes the truegraph
                      fitpost_a = l_postres[[1]][[3]][[1]],
                      fitpost_b = l_postres[[1]][[3]][[2]],
                      fitemp_a = l_res[[1]][[3]][[1]],
                      fitemp_b = l_res[[1]][[3]][[2]],
                      n_datasets = 100,
                      comparison = "frob",
                      n_draws = 1000)


wc_res %>% 
  filter(mat == "beta") %>% 
  # pivot_longer(cols = c("null", "emp"), names_to = "res") %>% 
  ggplot(aes(x = null))+
  geom_density()+
  geom_vline(aes(xintercept = emp))


```

Do this for all models.
```{r}
# All possible model combinations
comp_combinations <- expand_grid_unique(mod_a = seq(1, n_ind, 1),
                                 mod_b = seq(1, n_ind, 1))
# number of data-generating processes
n_dgp <- 3
# Number of comparison types
n_c_types <- 3

# Number of different changed graph structures
n_g_types <- 7

# Number of different tp conditions
n_t_types <- 4

dgp_names <- c("graph1", "graph2", "graph3")

change_names <- c("truegraph", "change_1.2", "change_1.4", "change_1.6",
                  "noise_0.1", "noise_0.2", "noise_0.3")


comp_names <- c("frob","maxdiff", "l1")

# Number of comparison combinations
n_c_comb <- nrow(comp_combinations)

# Setup storage
l_comp_within<- list()

# Loop over data-generating processes
for(d in seq(n_dgp)){
  l_comp_within[[d]] <- list()
  # Loop over timepoint conditions
  for(t in seq(n_t_types)){
    l_comp_within[[d]][[t]] <- list() 
    # Loop over changed graphs
    for(c in seq(n_g_types)){
      l_comp_within[[d]][[t]][[c]] <- list()
      # Loop over model combinations
      for(m in seq(n_c_comb)){
        l_comp_within[[d]][[t]][[c]][[m]] <- list()
        # Loop over comparison types
        for(c_type in seq(n_c_types)){
          l_comp_within[[d]][[t]][[c]][[m]][[c_type]] <- list()
          l_comp_within[[d]][[t]][[c]][[m]][[c_type]]$res <- within_compare(
                      mod_a = comp_combinations[m, 1],
                      mod_b = comp_combinations[m, 2],
                      # [[1]] indexes the truegraph
                      fitpost_a = l_postres[[d]][[t]][[1]],
                      fitpost_b = l_postres[[d]][[t]][[c]],
                      fitemp_a = l_res[[d]][[t]][[1]],
                      fitemp_b = l_res[[d]][[t]][[c]],
                      n_datasets = 100,
                      n_draws = 1000,
                      comparison = comp_names[c_type])
         # Store relevant parameters for evaluation
         l_comp_within[[d]][[t]][[c]][[m]][[c_type]]$params <- list(
           dgp = dgp_names[d],
           tp = n_tp[t],
           comp_graph = change_names[c]
         )
        } 

        
        
      }
    }
  }
}


```

Now evaluate this: 
```{r}
within_compare_eval(l_res = l_comp_within[[1]][[3]][[4]][[1]][[1]])


# Set up for all simulation conditions
l_eval_within <- list()
# Loop over data-generating processes
# TODO change back to seg(n_dgp)
for(d in seq(n_dgp)){
  l_eval_within[[d]] <- list()
  # Loop over timepoint conditions
  for(t in seq(n_t_types)){
    l_eval_within[[d]][[t]] <- list() 
    # Loop over changed graphs
    for(c in seq(n_g_types)){
      l_eval_within[[d]][[t]][[c]] <- list()
      # Loop over model combinations
      for(m in seq(n_c_comb)){
        l_eval_within[[d]][[t]][[c]][[m]] <- list()
        l_eval_within[[d]][[t]][[c]][[m]] <- lapply(l_comp_within[[d]][[t]][[c]][[m]],within_compare_eval)

      }
    }
  }
}


# Now unpack
df_eval_within <- rrapply::rrapply(l_eval_within, how = "bind")


```


Plot
```{r}
df_eval_within %>% 
  pivot_longer(cols = c("beta_a", "beta_b", "pcor_a", "pcor_b"), names_to = "res") %>% 
  # mutate(across(c(comp, dgp, tp, comp_graph, res),
                # ~ as.factor(.))) %>% 
  filter(dgp == "graph1") %>% 
  # comp_graph == truegraph is comparison with same dgp, so null
  filter(comp_graph != "truegraph") %>% 
  mutate(sig05 = ifelse(value < 50, 1, 0),
         sig01 = ifelse(value < 10, 1, 0)) %>%
  group_by(comp, tp, comp_graph, res) %>%
  mutate(n_comp = n()) %>%  
  summarize(power05 = sum(sig05)/n_comp) %>% 
  # somehow does not cut everything if I don't do this
  summarize(power05 = mean(power05)) %>% 
  ggplot(aes(x = tp, y = power05, col = comp)) + 
  geom_line()+
  facet_grid(comp_graph ~ res)+
  theme_minimal()+
  scale_color_okabe_ito()



```



# New Approach (25.01.2023): Posterior Comparison without Predictive
The idea is to compare distances between matrices in the posterior as a reference distribution, without having to compute any posterior predictives.

Compare all models:
```{r}
# Setup storage
l_comp_within <- list()

# Setup list of conditions
# All possible model combinations
comp_combinations <- expand_grid_unique(mod_a = seq(1, n_ind, 1),
                                 mod_b = seq(1, n_ind, 1))
# number of data-generating processes
n_dgp <- 3
# Number of comparison types
n_c_types <- 3

# Number of different changed graph structures
n_g_types <- 7

# Number of different tp conditions
n_t_types <- 4

dgp_names <- c("graph1", "graph2", "graph3")

change_names <- c("truegraph", "change_1.2", "change_1.4", "change_1.6",
                  "noise_0.1", "noise_0.2", "noise_0.3")


comp_names <- c("frob","maxdiff", "l1")

comp_grid_mod <- expand.grid(mod_a = seq(1, n_ind, 1),
            mod_b = seq(1, n_ind, 1),
            dgp = dgp_names, 
            change = change_names,
            tp = n_tp,
            comp = comp_names)

# Filter redundant comparisons within truegraph
comp_grid_mod <- comp_grid_mod %>% 
  filter(!c(mod_a == mod_b & change == "truegraph")) 


# Number of comparison combinations
n_c_comb <- nrow(comp_grid_mod)


# as many clusters as n_g_types
cl = makeCluster(n_g_types)
registerDoParallel(cl)
# Loop over data-generating processes
for(d in seq(n_dgp)){
  l_comp_within[[d]] <- list()
  # Loop over timepoint conditions
  for(t in seq(n_t_types)){
    l_comp_within[[d]][[t]] <- list() 
    # Loop over changed graphs
    for(c in seq(n_g_types)){
      l_comp_within[[d]][[t]][[c]] <- list()
      # Loop over model combinations
      for(m in seq(n_c_comb)){
        l_comp_within[[d]][[t]][[c]][[m]] <- list()
        # Loop over comparison types
        for(c_type in seq(n_c_types)){
          l_comp_within[[d]][[t]][[c]][[m]][[c_type]] <- list()
          l_comp_within[[d]][[t]][[c]][[m]][[c_type]]$res <- within_compare(
                      mod_a = comp_combinations[m, 1],
                      mod_b = comp_combinations[m, 2],
                      # [[1]] indexes the truegraph
                      fitpost_a = l_res[[d]][[t]][[1]],
                      fitpost_b = l_res[[d]][[t]][[c]],
                      fitemp_a = l_res[[d]][[t]][[1]],
                      fitemp_b = l_res[[d]][[t]][[c]],
                      n_datasets = 100,
                      n_draws = 1000,
                      comparison = comp_names[c_type],
                      postpred = FALSE)
         # Store relevant parameters for evaluation
         l_comp_within[[d]][[t]][[c]][[m]][[c_type]]$params <- list(
           dgp = dgp_names[d],
           tp = n_tp[t],
           comp_graph = change_names[c]
         )
        } 

        
        
      }
    }
  }
}

# write_rds(l_comp_within, "l_comp_within.RDS")


```


### New comparison with loading files
Previous model objects became too large, now we read them in instead of keeping them in memory.
```{r}
# TODO
# Think of how we can do this efficiently without re-loading data too often
# Probably loop over all the truegraphs and then all the changed ones
comp_grid <- expand.grid(dgp = dgp_names, 
            change = change_names, 
            comp = comp_names,
            tp = n_tp)
  
# Add filenames
comp_grid <- comp_grid %>% 
  mutate(ref_file = paste0("fit_", dgp, "_","truegraph","_",tp,".RDS"),
         comp_file = paste0("fit_", dgp, "_",change,"_",tp,".RDS"))


# for testing
comp_grid <- expand.grid(dgp = "graph1", 
            change = change_names, 
            comp = comp_names,
            tp = n_tp)
  
# Add filenames
comp_grid <- comp_grid %>% 
  mutate(ref_file = paste0("fit_", dgp, "_","truegraph","_",tp,".RDS"),
         comp_file = paste0("fit_", dgp, "_",change,"_",tp,".RDS"))


lapply(c(1:nrow(comp_grid)), function(n){
  
  ## Get args
  dgp_it <- comp_grid$dgp[n]
  change_it <- comp_grid$change[n]
  tp_it <- comp_grid$tp[n]
  
  ## Read files
  
  l_ref <- readRDS(file = here::here(paste0("data/compare_sim_data/", comp_grid$ref_file[n])))
  l_comp <- readRDS(file = here::here(paste0("data/compare_sim_data/", comp_grid$comp_file[n])))

  ## Perform comparison 
  # Loop over all possible model combinations
  comp_ind <- comp_grid_mod %>% 
    filter(as.character(dgp) == as.character(dgp_it) &
           as.character(change) == as.character(change_it) & 
           as.numeric(as.character(tp)) == as.numeric(as.character(tp_it)))
    
  wc_res <- list()
  lapply(c(1:nrow(comp_ind)), function(x){
    wc_res[[x]] <- list()
    wc_res[[x]] <- within_compare(mod_a = comp_ind$mod_a[x],
                      mod_b = comp_ind$mod_b[x],
                      fitpost_a = l_ref,
                      fitpost_b = l_comp,
                      fitemp_a = l_ref,
                      fitemp_b = l_comp,
                      n_datasets = 100,
                      n_draws = 1000,
                      comparison = comp_ind$comp[x],
                      postpred = FALSE)
  })
  print(wc_res)
  
  # Save output
  return(wc_res)
  
  
})




l_ref <- readRDS(file = here::here(paste0("data/compare_sim_data/", comp_grid$ref_file[1])))
l_comp <- readRDS(file = here::here(paste0("data/compare_sim_data/", comp_grid$comp_file[1])))


within_compare(mod_a = 1,
                      mod_b = 2,
                      fitpost_a = l_ref,
                      fitpost_b = l_comp,
                      fitemp_a = l_ref,
                      fitemp_b = l_comp,
                      n_datasets = 100,
                      n_draws = 1000,
                      comparison = "frob",
                      postpred = FALSE)

```





Visualize reference distributions and empirical value
```{r}
plot_test <- function(comp_obj,
                      modmat,
                      ref_dist = null,
                      emp_diff = emp,
                      ind = model_ind,
                      comp_type = comp){
  
  # Store params
  pr <- comp_obj$params
  
  # get comp type
  ct <- comp_obj$res %>% distinct({{comp_type}})
  
  # Get matrix as character
  c_matrix <- deparse(substitute(modmat))

  
  comp_obj$res %>% 
    filter(mat == c_matrix) %>%  
    ggplot()+
    geom_histogram(aes(x = {{ref_dist}}, fill = as.factor({{ind}})), alpha = 0.65,  position = "identity", bins = 100)+
    geom_vline(aes(xintercept = max({{emp_diff}})))+
    theme_minimal()+
    labs(x = paste0(ct, " Norm Value"),
         fill = "Model",
         caption = paste0("DGP: ", pr$dgp,", TP: ", pr$tp, ", Comparison Graph: ", pr$comp_graph, ", Matrix: ", c_matrix))+
    ggokabeito::scale_fill_okabe_ito()

}

pdf(file = "figures/distributions/reference_dists_xlim10.pdf")
for(i in 1:4){
  for(j in 1:7){
    for(c in 1:3){
          b <- plot_test(l_comp_within[[1]][[i]][[j]][[5]][[c]], modmat = beta)
          plot(b)
          p <- plot_test(l_comp_within[[1]][[i]][[j]][[5]][[c]], modmat = pcor)
          plot(p)      
      }
  }
}
dev.off()


```

Then, try approximating the norm distribution with known densities. 
```{r}
ref_dist <- l_comp_within[[1]][[3]][[6]][[5]][[1]]$res %>% 
  as.data.frame() %>% 
  filter(mat == "beta") %>% 
  filter(model_ind == 1) %>% 
  pull(null)

hist(ref_dist, breaks = 40)


cs_res<- MASS::fitdistr(ref_dist, densfun = "chi-squared", start = list(df = 1))
no_res <- MASS::fitdistr(ref_dist, densfun = "normal")

```






Evaluate all models:
```{r}
# Set up for all simulation conditions
l_eval_within <- list()
# Loop over data-generating processes

for(d in seq(n_dgp)){
  l_eval_within[[d]] <- list()
  # Loop over timepoint conditions
  for(t in seq(n_t_types)){
    l_eval_within[[d]][[t]] <- list() 
    # Loop over changed graphs
    for(c in seq(n_g_types)){
      l_eval_within[[d]][[t]][[c]] <- list()
      # Loop over model combinations
      for(m in seq(n_c_comb)){
        l_eval_within[[d]][[t]][[c]][[m]] <- list()
        l_eval_within[[d]][[t]][[c]][[m]] <- lapply(l_comp_within[[d]][[t]][[c]][[m]],within_compare_eval)

      }
    }
  }
}


# Now unpack
df_eval_within <- rrapply::rrapply(l_eval_within, how = "bind")



```


Plot results.
TODO: This needs to take the amount of nonconverged models into account
```{r}
plot_within_emppost <- df_eval_within %>% 
  pivot_longer(cols = c("beta_a", "beta_b", "pcor_a", "pcor_b", "kappa_a", "kappa_b"), names_to = "res") %>% 
  # mutate(across(c(comp, dgp, tp, comp_graph, res),
                # ~ as.factor(.))) %>% 
  filter(dgp == "graph1") %>% 
  # comp_graph == truegraph is comparison with same dgp, so null
  # filter(comp_graph != "truegraph") %>% 
  mutate(sig05 = ifelse(value < 50, 1, 0),
         sig01 = ifelse(value < 10, 1, 0)) %>%
  group_by(comp, tp, comp_graph, res) %>%
  mutate(n_comp = n()) %>%  
  summarize(power05 = sum(sig05)/n_comp) %>% 
  # somehow does not cut everything if I don't do this
  summarize(power05 = mean(power05)) %>% 
  ggplot(aes(x = tp, y = power05, col = comp)) + 
  geom_line()+
  facet_grid(comp_graph ~ res)+
  theme_minimal()+
  scale_color_okabe_ito()

ggsave("plot_within_emppost2.svg", plot_within_emppost, device = "svg", path = here("figures/"))  


```

