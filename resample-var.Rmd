---
title: "var-resample"
author: "Bj√∂rn Siepe"
date: "2022-11-10"
output: html_document
---

# Background

Goal is to compare if two VAR models $A$ and $B$ are really different from another. We use resampling to do this. \## Idea: 1. Fit $A$ and $B$ to their respective data. 2. Using parameters of $A$, generate $n$ new time series. 3. Refit $A$ and $B$ on each of the $n$ time series. 4. Obtain measure of fit, such as $MSE$. 5. Compare error distributions, either by cutoff or something like a Divergence. (6. Maybe repeat the other way around, so sampling from $B$?)

## Possible Issues:

We lose uncertainty in step 2 if we resample from a time series without accounting for the uncertainty in the parameter estimates, so maybe we could resample from the posterior? Then it would just be posterior predictive distribution, right?

# Two models, same data

If we fit two models on the same data, they should be almost identical. We do not regularize here at first. So our distribution of resampled MSEs should be very similar.

```{r preparations, include = FALSE}

library(tidyverse)
library(graphicalVAR)
library(doParallel)
library(mgm)
library(mlVAR)
library(BGGM)
library(reshape2)      # Data manipulation
library(mvtnorm)       # Sim from posterior
library(stats)         # KS-Test
library(philentropy)   # divergence measures
library(todor)         # keep on track with stuff to do
```

Another idea would be to use empirical graphs as basis, e.g. from Hoekstra paper. 

```{r}
# Ground truth model is still generated with graphicalVAR 
set.seed(2022)
graph <- randomGVARmodel(Nvar = 6, probKappaEdge = 0.1, probBetaEdge = .5)
dat <- graphicalVARsim(200, beta = graph$beta, kappa = graph$kappa)
dat2 <- graphicalVARsim(200, beta = graph$beta, kappa = graph$kappa)

# # Do this m times
# l_raw <- list()
# n_mod <- 100   # number of models to create
# for(i in 1:n_mod){
#   l_raw[[i]] <- as.data.frame(graphicalVAR::graphicalVARsim(nTime = 200,
#                                 beta = graph$beta,
#                                 kappa = graph$kappa))
#   
#   
# }
# 
# 
# 
# # use different dgp for dat3
# set.seed(2022)
# graph3 <- randomGVARmodel(Nvar = 6, probKappaEdge = 0.1, probBetaEdge = .3)
# dat3 <- graphicalVARsim(200, beta = graph3$beta, kappa = graph$kappa)




```

# Bayesian approach

Idea: Use the posterior distribution to obtain a variance-covariance matrix of the parameters, then sample 100 different datasets from these parameters?

```{r}
# Fit the model
rho_sd <- 0.5
beta_sd <- 1
seed <- 2022
n_iter <- 5000

l_res <- list()
for(i in seq(n_mod)){
  l_res[[i]] <- BGGM::var_estimate(l_raw[[i]],
                                   rho_sd = rho_sd,
                                   beta_sd = beta_sd,
                                   iter = n_iter,
                                   progress = FALSE,
                                   seed = seed)

}


```

## Directly use posterior samples

Take the first 100 samples from the posterior of the first model (after deleting warmup samples).
```{r}
# posterior sapmles
ps <- l_res[[1]]$fit$beta[,,51:5050]

# setup foreach for data generation
ncores = parallel::detectCores() - 4
cl = makeCluster(ncores)
registerDoParallel(cl)

# TODO: maybe use own function to simulateVAR model, this gives me weird results
# Could also use sth. inspired by here: https://github.com/jmbh/ARVAR/blob/master/aux_functions.R

# number of samples to create
n = 100
l_dat <- foreach(i = seq(n), .packages = "mlVAR") %dopar% {
  mod <- simulateVAR(pars = ps[,,i],
                     means = 0,
                     Nt = 200,
                     residuals = 1)     # need to check if I should provide residual covariance matrix here
  mod
  
}

stopCluster(cl)

# Convert data to BGGM format
# TODO Something here does not work? The last Y value gets cut off because 
# we insert NA for the lagged value matrix
# Need to think about whether I keep it this way to comply with Williams or I change it
source("aux_funs.R")
l_dat <- lapply(l_dat, format_bggm)
write_rds(l_dat, "l_dat_simvar.RDS")
```

Refit all models to all 100 datasets. Use the internal function by Williams, modified by me.

Do I even need the intercept when all time series are standardized? He seems to ignore them. I also still need to understand what the 'predict' function does here and if it is even useful for me. The predict function uses the posterior samples to predict fitted values, so it even contains the foundation for the new simulated datasets? Not sure if this makes sense. Here we are however interested in the comparison of the graph that will actually be plotted, so just use the posterior mean of the beta coefficients for prediction.

```{r}

ncores = parallel::detectCores() - 4
cl = makeCluster(ncores)
registerDoParallel(cl)

l_refit <- foreach(i = seq(n_mod), .export = c("predict_pmu.var_estimate"))  %dopar% { # each model
  l_m_ref <- list()  # store for each model
  for(j in 1:n){   # each simulated dataset
    l_m_ref[[j]] <- predict_pmu.var_estimate(object = l_res[[i]],
                             data = l_dat[[j]])
  }
  return(l_m_ref)
} 



stopCluster(cl)
```

We use RMSE to evaluate the fit, but could also use Bayesian $R^2$ or similar stuff (maybe even the likelihood). As we get credible intervals for the predictions, we could also do something with structure recovery, so see which edges were included and which weren't if we opt for some kind of sparsity. 

```{r}
l_rmse <- list()
ncores = parallel::detectCores() - 4
cl = makeCluster(ncores)
registerDoParallel(cl)
l_rmse <- foreach(i = seq(n_mod)) %dopar% {  # for every model 
  df_rmse <- data.frame(iter = 1:n,
                    rmse = rep(NA, n),
                    model = rep(i, n))
  for(j in 1:n){      # for every dataset
    df_rmse[j,"rmse"] <- f_eval_refit(data = l_dat[[j]],
               refit = l_refit[[i]][[j]])
  }
  return(df_rmse)
}

stopCluster(cl)
df_errors <- do.call(rbind, l_rmse)
write_rds(df_errors, "df_errors.Rds")
```


Plot the error distribution:
```{r}
df_errors %>% 
  filter(model < 10) %>% 
  ggplot(aes(x = rmse, group = model , fill = as.factor(model), col = as.factor(model)))+
  ggdist::stat_halfeye(alpha = 0.7)+
  theme_minimal()+
  ggokabeito::scale_fill_okabe_ito()+
  ggokabeito::scale_color_okabe_ito()

```

Two ways of numerical comparison: First, treat as empirical distribution and use Kolmogorow-Smirnow. P-Values can be computed exactly or with Monte Carlo Sampling, I need to check which one makes the most sense. Apparently, the KS test (and similar tests) assume independence between the curves under consideration. 
```{r}
err1 <- df_errors$rmse[df_errors$model == 1]
err2 <- df_errors$rmse[df_errors$model == 2]
err3 <- df_errors$rmse[df_errors$model == 3]

stats::ks.test(x = err1, 
               y = err2,
               alternative = "less") # test one sided if first model is better

stats::ks.test(x = err1,
               y = err3) # interesting case on why we should not use one-sided testing: other model predicts better!


# Loop over test statistics
# Baseline is model 1
err1 <- df_errors$rmse[df_errors$model == 1]
df_ks_res <- data.frame(model = rep(NA, n_mod),
                        statistic = rep(NA, n_mod),
                        pval = rep(NA, n_mod))
options(scipen=999)
for(m in 2:n_mod){
  err_alt <- df_errors$rmse[df_errors$model == m]
  tmp <- stats::ks.test(x = err1,
                 y = err_alt)
  df_ks_res[m,"model"] <- m
  df_ks_res[m, "statistic"] <- tmp$statistic[[1]]
  df_ks_res[m, "pval"] <- round(tmp$p.value, 5)

}

# Still too many false positives?
sum(df_ks_res$pval < 0.01, na.rm = TRUE)



```

Second, estimate density, treat as empirical distribution and calculate Jenson-Shannon-Divergence (which is symmetrical, KL-Divergence is not?).
The exact method for estimating the density should not matter too much, as the distribution of errors should roughly follow a normal distribution. For now, just use the density approach in the `stats` package with a Gaussian kernel.
```{r}
dens1 <- density(err1)
dens1 <- data.frame(x = dens1$x,
                       y = dens1$y)
dens2 <- density(err2)
dens2 <- data.frame(x = dens2$x,
                    y = dens2$y)

ggplot()+
  geom_line(data = dens1, aes(x = x, y = y))+
  geom_line(data = dens2, aes(x = x, y = y), lty = 2)


test <- rbind(dens1$y, dens2$y)
philentropy::KL(test)
# does not work because the rows do not sum to 1
# I need to integrate first 
# or I just use ecdf() to generate empirical distribution from the raw data without estimating density first...
ecdf1 <- stats::ecdf(err1)
p1 <- ecdf1(err1)
ecdf2 <- stats::ecdf(err2)
p2 <- ecdf2(err2)
x <- rbind(p1,p2)

# compute similarity between distributions
1-philentropy::JSD(x)


# compute for whole dataset
jsd_res <- f_comp_jsd()


```

Another idea: use a commonly understandable metric. As we have paired error statistics (two models on the same data), we can just compute the probability that a given model fits as good or better as the reference model. This does not say anything about the magnitude of the difference, which is why models that fit slightly worse might fare badly in this approach. I don't think this is a good idea anymore. 

Another idea: compute something like absolute difference between network results, as van Borkulo et al. did it. 

```{r}

```



For later, obtain the likelihoods of each model directly and then compare these across samples. 
```{r}

```




# Concatenating Time Series
Another idea is based on Williams & Mulder (2020). Instead of comparing the posterior predictive estimations of two datasets directly, we could concatenate two time series under the null hypothesis that they originate from the same data-generating process. We could then compare the posterior predictive results from these concatenated time series to either a distribution from an individual time series or just a single test statistic. This is also comparable to Zhang, Templin & Mintz (2022). 

```{r concatenate-ts}
# time series need to be standardized before concatenation
# we also need to assume stationarity 
# first just take the first two time series here
conc_12 <- rbind(l_dat[[1]]$Y, l_dat[[2]]$Y)

# delete first entry of second time series to create gap
conc_12[nrow(conc_12)/2+1,] <- c(NA, NA, NA, NA, NA, NA)

# Estimate new model on concatenated dataset
# Fit the model
rho_sd <- 0.5
beta_sd <- 1
seed <- 2022
n_iter <- 5000

# estimate model on concatenated dataset
res_conc12 <- BGGM::var_estimate(conc_12,
                                   rho_sd = rho_sd,
                                   beta_sd = beta_sd,
                                   iter = n_iter,
                                   progress = FALSE,
                                   seed = seed)

# results are very different compared to just one model



```




















# Reliability estimation from posterior samples

Compare variance within a person (across posterior samples) to variation across individuals


```{r data-simulation}
# Use one ground-truth model and simulate from it (assuming homogeneity)
set.seed(2022)
graph <- randomGVARmodel(Nvar = 6, probKappaEdge = 0.1, probBetaEdge = .5)

# simulate 50 data sets
l_sims <- list()
for(i in 1:250){
  l_sims[[i]] <- graphicalVAR::graphicalVARsim(nTime = 200,
                                          beta = graph$beta,
                                          kappa = graph$kappa)
}

```

Now we fit the bayesian VAR model again.

```{r}
l_est <- list()
l_sims <- lapply(l_sims, as.data.frame)
for(i in 1:250){
  l_est[[i]] <- try(BGGM::var_estimate(l_sims[[i]],
                                   seed = 2022,
                                   progress = FALSE))
}


```

Obtain variance-covariance matrices for posterior sampling distributions: I still have to understand what the order means, so what is lagged on what.

```{r}
ncores = parallel::detectCores() - 4
cl = makeCluster(ncores)
registerDoParallel(cl)
source("aux_funs.R")
l_cov <- foreach(i = 1:250) %dopar% {
  covres <- try(f_postcov(l_est[[i]])) 
  covres
}


stopCluster(cl)
```

Save mean betas and covariances for each individual:

```{r}
l_beta <- list()
for(i in 1:250){
  beta <- melt(as.matrix(l_est[[i]]$beta_mu))
  beta$est <- paste(beta$Var1, beta$Var2, sep = "_")
  beta$id <- i
  beta <- subset(beta, select = -c(Var1, Var2))
  l_beta[[i]] <- beta
}
df_beta <- do.call(rbind, l_beta)

# Variance-covariance in long format
l_cov_long <- list()
for(i in 1:250){
  cov <- melt(as.matrix(l_cov[[i]]))
  cov$cov <- paste(cov$Var1, cov$Var2, sep = "_")
  cov$id <- i
  cov <- subset(cov, select = -c(Var1, Var2))
  l_cov_long[[i]] <- cov
}
df_cov <- do.call(rbind, l_cov_long)

# Only use variances for now
pars <- unique(df_beta$est)
variance <- paste(pars, pars, sep = "_")  # covariance of parameters with themselves, so variances

# filter for variance parameters, rename to remove redundancy
df_var <- df_cov %>% 
  filter(cov %in% variance) %>% 
  mutate(cov = stringr::str_sub(cov, 1, 8))
  

```



Compute reliability as within deviation over between deviation:

```{r}
# variance of betas across individuals
var_bw <- df_beta %>% 
  group_by(est) %>% 
  summarize(var_beta = var(value)) %>% 
  summarize(mean_var_beta = mean(var_beta)) %>% 
  pull(mean_var_beta)

# mean variance of betas within individuals
var_wi <- df_var %>% 
  group_by(cov) %>% 
  summarize(var_cov = mean(value)) %>% 
  summarize(mean_var_cov = mean(var_cov)) %>% 
  pull(mean_var_cov)

rel <- (var_bw-var_wi)/(var_bw)
# This is a bad result. A large part of the variance is attributed
# to the between-person level, although it should just be sampling noise

var_bw/var_wi

```


Other idea:

-means per var per person, variance between means between persons
-draw 10 values per person, compute SD

for each person: descriptive mean and sampling error, true mean should be the same
- then we can test if the formula works well

simulate one regression per person
- 2 predictors, but predictors are multicollinear
- this affects standard errors, maybe this formula does not work here
- 
show Ricci stuff







