---
title: "var-resample"
author: "Björn Siepe"
date: "2022-11-10"
output: html_document
---
# Background
Goal is to compare if two VAR models $A$ and $B$ are really different from another. We use resampling to do this.
## Idea: 
1. Fit $A$ and $B$ to their respective data. 
2. Using parameters of $A$, generate $n$ new time series.
3. Refit $A$ and $B$ on each of the $n$ time series.
4. Obtain measure of fit, such as $MSE$. 
5. Compare error distributions, either by cutoff or something like a Divergence. 
(6. Maybe repeat the other way around, so sampling from $B$?)


## Possible Issues:
We lose uncertainty in step 2 if we resample from a time series without accounting for the uncertainty in the parameter estimates, so maybe we could resample from the posterior? Then it would just be posterior predictive distribution, right?


# Two models, same data
If we fit two models on the same data, they should be almost identical.
We do not regularize here at first. 
So our distribution of resampled MSEs should be very similar. 
```{r preparations, include = FALSE}

library(tidyverse)
library(graphicalVAR)
library(doParallel)
library(mgm)
library(mlVAR)
library(BGGM)
library(reshape2)
library(mvtnorm)
```


I will just use mgm and mlVAR for model fitting and simulating now. 
```{r}
# Ground truth model is still generated with graphicalVAR 
set.seed(2022)
graph <- randomGVARmodel(Nvar = 6, probKappaEdge = 0.1, probBetaEdge = .5)
dat <- graphicalVARsim(200, beta = graph$beta, kappa = graph$kappa)

# Fit VAR models

model_a <- mgm::mvar(data = dat,
                     type = rep("g", 6),
                     level = rep(1,6),
                     lambdaSel = "EBIC",
                     lambdaGam = 0,
                     lags = 1,
                     consec = 1:nrow(dat),
                     threshold = "none", 
                     scale = TRUE)


model_b <- mgm::mvar(data = dat,
                     type = rep("g", 6),
                     level = rep(1,6),
                     lambdaSel = "EBIC",
                     lambdaGam = 0,
                     lags = 1,
                     consec = 1:nrow(dat),
                     threshold = "none", 
                     scale = TRUE)

ntime_a <- length(model_a$call$data_lagged$included)
pars_a <- model_a$wadj[,,1]
int_a <- unlist(model_a$intercepts)

# the intercepts are so small that the data afterwards becomes weird
# so I manually set the intercepts
int_a <- c(-0.3, 0.1, 0.05, -0.25, 0.4, 0.01)

```




Now we simulate 100 models from the parameters of $A$. 
```{r}
# setup parallelization
ncores = parallel::detectCores() - 4
cl = makeCluster(ncores)
registerDoParallel(cl)

# somehow, the following gives me insanely weird data, basically everything is almost zero
l_dat <- foreach(i = 1:100, .packages = "mlVAR") %dopar% {
  mod <- simulateVAR(pars = pars_a,
                     means = 0,
                     Nt = ntime_a,
                     residuals = 1)     # need to check if I should provide residual covariance matrix here
  mod
  
}


# fit model A to the data 100 times
l_res_a <- foreach(i = 1:100, .packages = "mgm") %dopar% {
  res <- predict(object = model_a, 
                 data = l_dat[[i]])
  errors <- res$errors
}

# slightly change model B and fit to the data again
for(i in 1:nrow(model_b$wadj[,,1])){
  for(j in 1:ncol(model_b$wadj[,,1])){
    if(model_b$wadj[,,1][i,j]>0.0001 | model_b$wadj[,,1][i,j] < -0.0001){
      model_b$wadj[,,1][i,j] <- model_b$wadj[,,1][i,j] + rnorm (n = 1,mean = 0.05, sd = 0.01)
    }
  }
}

l_res_b <- foreach(i = 1:100, .packages = "mgm") %dopar% {
  res <- predict(object = model_b, 
                 data = l_dat[[i]])
  errors <- res$errors
}


stopCluster(cl)
```

This does not give a nice distribution with uncertainty.
It also gives me negative R² values, which is really weird, probably
due to the insanely small variance?
Maybe I should resample from the VAR model using the uncertainty
in $\beta$ and then compare distributions. 



# Bayesian approach
Idea: Use the posterior distribution to obtain a variance-covariance
matrix of the parameters, then sample 100 different datasets from these parameters?
```{r}
# Fit the model
dat <- as.data.frame(dat)
bay_res <- var_estimate(dat)
```
Then obtain characteristics of the posterior: 
```{r}
beta_posterior <- bay_res$fit$beta
# delete warm-up samples
beta_posterior <- beta_posterior[,,51:5050]

# get mean and SD of posterior estimates
beta_mu <- round(apply(beta_posterior,1:2,mean), digits = 3)
beta_sd <- round(apply(beta_posterior,1:2,sd), digits = 3)

# obtain the covariance matrix of estimates
dimnames(beta_posterior)[[1]] <- c("V1", "V2", "V3", "V4", "V5", "V6")
dimnames(beta_posterior)[[2]] <- c("V1", "V2", "V3", "V4", "V5", "V6")

# convert array to list
l_beta_posterior <- lapply(seq(dim(beta_posterior)[3]), function(x) beta_posterior[,,x])

ldf_beta_posterior <- lapply(l_beta_posterior, function(x){melt(as.matrix(x))})

# keep sample index
df_beta_posterior <- map_dfr(ldf_beta_posterior, .f = rbind, .id = "index")

# pivot wider to obtain cov-matrix of predictors across posterior samples
vcov_beta <- df_beta_posterior %>% 
  pivot_wider(id_cols = index,
              names_from = c(Var1, Var2)) %>% 
  dplyr::select(!index) %>% 
  cov()
```

Try simulating from this matrix. 
```{r}
ppd <- mvtnorm::rmvnorm(100, mean = beta_mu, sigma = vcov_beta)

# attach names from vcov_beta


```



## Directly use posterior samples
Do I even need to sample from the posterior samples again? A bit confusing.
Maybe I just take the first 100 samples from the posterior. 
```{r}
dgp <- beta_posterior[,,1:100]

# setup foreach for data generation
ncores = parallel::detectCores() - 4
cl = makeCluster(ncores)
registerDoParallel(cl)

# TODO: maybe use own function to simulateVAR model, this gives me weird results

l_dat <- foreach(i = 1:100, .packages = "mlVAR") %dopar% {
  mod <- simulateVAR(pars = dgp[,,i],
                     means = 0,
                     Nt = 200,
                     residuals = 1)     # need to check if I should provide residual covariance matrix here
  mod
  
}

stopCluster(cl)

# Convert data to BGGM format
source("aux_funs.R")
l_dat <- lapply(l_dat, format_bggm)
write_rds(l_dat, "l_dat_simvar.RDS")
```


Refit the model to all 100 datasets. Use the internal function by Williams, modified by me.

Do I even need the intercept when all time series are standardized? He seems
to ignore them. I also still need to understand what the 'predict' function does here
and if it is even useful for me.
The predict function uses the posterior samples to predict fitted values, so it even
contains the foundation for the new simulated datasets? Not sure if this makes sense.
```{r}
# 
# ncores = parallel::detectCores() - 4
# cl = makeCluster(ncores)
# registerDoParallel(cl)
# l_refit <- foreach(i = 1:100) %dopar% {
#   source("aux_funs.R")
#   ref <- predict(object = bay_res,
#                  data = l_dat[[i]])
#   
# } 
# 
# 
# stopCluster(cl)
```

However, the previous method uses posterior samples to evaluate predictions. Here we are however interested in the comparison of the graph that will actually be plotted, so just use the posterior mean of the beta coefficients for prediction.
```{r}

ncores = parallel::detectCores() - 4
cl = makeCluster(ncores)
registerDoParallel(cl)
l_refit <- foreach(i = 1:100) %dopar% {
  source("aux_funs.R")
  ref <- predict_pmu.var_estimate(object = bay_res,
                 data = l_dat[[i]])

}


stopCluster(cl)
```







Next, I need to write a function to evaluate the results.
Here we use RMSE, but could also use Bayesian $R^2$ or similar stuff. 
As we get credible intervals for the predictions, we could also do something
with structure recovery, so see which edges were included and which weren't.
```{r}
source("aux_funs.R")
# Get Prediction errors across all datasets
df_mse <- data.frame(iter = 1:length(l_dat),
                    mse = rep(NA, length(l_dat)))
for(i in 1:length(l_dat)){
  df_mse[i,"mse"] <- f_eval_refit(data = l_dat[[i]], refit = l_refit[[i]])
}
```

Now plot the error distribution:
```{r}
plot_error_dist(df_mse)
```


## Compare error distribution to slightly different model
Add small random noise to the coefficients of the "true" model and refit to data.
This is not trivial, because the "predict" function uses posterior samples
to evaluate the prediction and not just the posterior mean. Maybe I should rewrite the function so that just the mean is used to obtain predictions.
```{r}
# slightly change model B and fit to the data again
# only change nonzero coefs
bay_res_b <- bay_res
for(i in 1:nrow(bay_res_b$beta_mu)){
  for(j in 1:ncol(bay_res_b$beta_mu)){
    if(bay_res_b$beta_mu[i,j]!=0){
      bay_res_b$beta_mu[i,j] <- bay_res_b$beta_mu[i,j]+rnorm(1, mean = 0, sd = 0.25)
    }
  }
}

ncores = parallel::detectCores() - 4
cl = makeCluster(ncores)
registerDoParallel(cl)
l_refit <- foreach(i = 1:100) %dopar% {
  source("aux_funs.R")
  ref <- predict(object = bay_res_b,
                 data = l_dat[[i]])
  
} 


stopCluster(cl)
```








