---
title: "var-resample"
author: "Bj√∂rn Siepe"
date: "2022-11-10"
output: html_document
---
# Background
Goal is to compare if two VAR models $A$ and $B$ are really different from another. We use resampling to do this.
## Idea: 
1. Fit $A$ and $B$ to their respective data. 
2. Using parameters of $A$, generate $n$ new time series.
3. Refit $A$ and $B$ on each of the $n$ time series.
4. Obtain measure of fit, such as $MSE$. 
5. Compare error distributions, either by cutoff or something like a Divergence. 
(6. Maybe repeat the other way around, so sampling from $B$?)


## Possible Issues:
We lose uncertainty in step 2 if we resample from a time series without accounting for the uncertainty in the parameter estimates, so maybe we could resample from the posterior? Then it would just be posterior predictive distribution, right?


# Two models, same data
If we fit two models on the same data, they should be almost identical.
We do not regularize here at first. 
So our distribution of resampled MSEs should be very similar. 
```{r preparations, include = FALSE}

library(tidyverse)
library(graphicalVAR)
library(doParallel)
library(mgm)
library(mlVAR)
library(BGGM)
library(reshape2)
library(mvtnorm)
```


I will just use mgm and mlVAR for model fitting and simulating now. 
```{r}
# Ground truth model is still generated with graphicalVAR 
set.seed(2022)
graph <- randomGVARmodel(Nvar = 6, probKappaEdge = 0.1, probBetaEdge = .5)
dat <- graphicalVARsim(200, beta = graph$beta, kappa = graph$kappa)
```




# Bayesian approach
Idea: Use the posterior distribution to obtain a variance-covariance
matrix of the parameters, then sample 100 different datasets from these parameters?
```{r}
# Fit the model
dat <- as.data.frame(dat)
bay_res <- var_estimate(dat)
```
Then obtain characteristics of the posterior: 
```{r}
vcov_beta <- f_postcov(bay_res)

# Plot posterior
df_beta_posterior %>% 
  pivot_wider(id_cols = index,
              names_from = c(Var1, Var2)) %>% 
  dplyr::select(!index) %>% 
  ggplot(aes(x = V1_V1)) +
  geom_histogram(binwidth = 0.005)

```


## Directly use posterior samples
Do I even need to sample from the posterior samples again? A bit confusing.
Maybe I just take the first 100 samples from the posterior. 
```{r}
dgp <- beta_posterior[,,1:100]

# setup foreach for data generation
ncores = parallel::detectCores() - 4
cl = makeCluster(ncores)
registerDoParallel(cl)

# TODO: maybe use own function to simulateVAR model, this gives me weird results

l_dat <- foreach(i = 1:100, .packages = "mlVAR") %dopar% {
  mod <- simulateVAR(pars = dgp[,,i],
                     means = 0,
                     Nt = 200,
                     residuals = 1)     # need to check if I should provide residual covariance matrix here
  mod
  
}

stopCluster(cl)

# Convert data to BGGM format
source("aux_funs.R")
l_dat <- lapply(l_dat, format_bggm)
write_rds(l_dat, "l_dat_simvar.RDS")
```


Refit the model to all 100 datasets. Use the internal function by Williams, modified by me.

Do I even need the intercept when all time series are standardized? He seems
to ignore them. I also still need to understand what the 'predict' function does here
and if it is even useful for me.
The predict function uses the posterior samples to predict fitted values, so it even
contains the foundation for the new simulated datasets? Not sure if this makes sense.
```{r}
# 
# ncores = parallel::detectCores() - 4
# cl = makeCluster(ncores)
# registerDoParallel(cl)
# l_refit <- foreach(i = 1:100) %dopar% {
#   source("aux_funs.R")
#   ref <- predict(object = bay_res,
#                  data = l_dat[[i]])
#   
# } 
# 
# 
# stopCluster(cl)
```

However, the previous method uses posterior samples to evaluate predictions. Here we are however interested in the comparison of the graph that will actually be plotted, so just use the posterior mean of the beta coefficients for prediction.
```{r}

ncores = parallel::detectCores() - 4
cl = makeCluster(ncores)
registerDoParallel(cl)
l_refit <- foreach(i = 1:100) %dopar% {
  source("aux_funs.R")
  ref <- predict_pmu.var_estimate(object = bay_res,
                 data = l_dat[[i]])

}


stopCluster(cl)
```






Next, I need to write a function to evaluate the results.
Here we use RMSE, but could also use Bayesian $R^2$ or similar stuff. 
As we get credible intervals for the predictions, we could also do something
with structure recovery, so see which edges were included and which weren't.
```{r}
source("aux_funs.R")
# Get Prediction errors across all datasets
df_mse <- data.frame(iter = 1:length(l_dat),
                    mse = rep(NA, length(l_dat)))
for(i in 1:length(l_dat)){
  df_mse[i,"mse"] <- f_eval_refit(data = l_dat[[i]], refit = l_refit[[i]])
}
```

Now plot the error distribution:
```{r}
plot_error_dist(df_mse)
```




This is still TODO! 


# Reliability estimation from posterior samples
Compare variance within a person (across posterior samples) to variation
across individuals

What do I need?
- Multiple people
- Variance-Covariance Matrix of Posterior samples for each person
- Mean vector of posterior estimates

```{r data-simulation}
# Use one ground-truth model and simulate from it (assuming homogeneity)
set.seed(2022)
graph <- randomGVARmodel(Nvar = 6, probKappaEdge = 0.1, probBetaEdge = .5)

# simulate 50 data sets
l_sims <- list()
for(i in 1:50){
  l_sims[[i]] <- graphicalVAR::graphicalVARsim(nTime = 200,
                                          beta = graph$beta,
                                          kappa = graph$kappa)
}

```

Now we fit the bayesian VAR model again. 
```{r}
l_est <- list()
l_sims <- lapply(l_sims, as.data.frame)
for(i in 1:50){
  l_est[[i]] <- BGGM::var_estimate(l_sims[[i]],
                                   seed = 2022)
}


```

Obtain variance-covariance matrices for posterior sampling distributions:
I still have to understand what the order means, so what is lagged on what. 
```{r}
ncores = parallel::detectCores() - 4
cl = makeCluster(ncores)
registerDoParallel(cl)
l_cov <- foreach(i = 1:50) %dopar% {
  covres <- f_postcov(l_est[[i]]) 
  covres
}


stopCluster(cl)
```

Save mean betas and covariances for each individual:
```{r}
l_beta <- list()
for(i in 1:50){
  beta <- melt(as.matrix(l_est[[i]]$beta_mu))
  beta$est <- paste(beta$Var1, beta$Var2, sep = "_")
  beta$id <- i
  beta <- subset(beta, select = -c(Var1, Var2))
  l_beta[[i]] <- beta
}
df_beta <- do.call(rbind, l_beta)

# Variance-covariance in long format
l_cov_long <- list()
for(i in 1:50){
  cov <- melt(as.matrix(l_cov[[i]]))
  cov$cov <- paste(cov$Var1, cov$Var2, sep = "_")
  cov$id <- i
  cov <- subset(cov, select = -c(Var1, Var2))
  l_cov_long[[i]] <- cov
}
df_cov <- do.call(rbind, l_cov_long)

# Only use variances for now
pars <- unique(df_beta$est)
variance <- paste(pars, pars, sep = "_")  # covariance of parameters with themselves, so variances

# filter for variance parameters, rename to remove redundancy
df_var <- df_cov %>% 
  filter(cov %in% variance) %>% 
  mutate(cov = stringr::str_sub(cov, 1, 8))
  

```

Compute reliability as within deviation over between deviation: 
```{r}
# variance of betas across individuals
var_bw <- df_beta %>% 
  group_by(est) %>% 
  summarize(var_beta = var(value)) %>% 
  summarize(mean_var_beta = mean(var_beta)) %>% 
  pull(mean_var_beta)

# mean variance of betas within individuals
var_wi <- df_var %>% 
  group_by(cov) %>% 
  summarize(var_cov = mean(value)) %>% 
  summarize(mean_var_cov = mean(var_cov)) %>% 
  pull(mean_var_cov)

rel <- (var_bw-var_wi)/(var_bw)
# This is a bad result  lol. A large part of the variance is attributed
# to the between-person level, although it should just be sampling noise



```











