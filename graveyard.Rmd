---
title: "graveyard"
author: "Björn Siepe"
date: "2022-11-16"
output: html_document
---
# Add noise to graphs manually
```{r}
# # # add slight change of one model parameter to the list
# graph2 <- graph
# graph2$beta[which.max(graph2$beta)] <- graph2$beta[which.max(graph2$beta)]*1.5
# graph2$beta[which.max(graph2$kappa)] <- graph2$beta[which.max(graph2$kappa)]*1.5
# 
# l_graph <- list(graph = graph,
#                 graph2 = graph2)
# # add stronger change of one model parameter
# graph2 <- graph
# graph2$beta[which.max(graph2$beta)] <- graph2$beta[which.max(graph2$beta)]*-1
# l_graph <- list(graph = graph, 
#                 graph2 = graph2)
# # # also change kappa but less strongly
# graph2 <- graph
# graph2$beta[which.max(graph2$beta)] <- graph2$beta[which.max(graph2$beta)]*1.8
# graph2$kappa[which.max(graph2$kappa)] <- graph2$kappa[which.max(graph2$kappa)]*1.8
# 
# 
# l_graph <- list(graph = graph,
#                 graph2 = graph2)
# also change kappa
# TODO Need to be careful here! Kappa may not be >1, maybe not even 1 (I have to partial other stuff out...)
# 
# graph2 <- graph
# graph2$beta[which.max(graph2$beta)] <- graph2$beta[which.max(graph2$beta)]*2
# graph2$kappa <- changemax_kappa(graph2$kappa, change = 2)
# Add random noise to the network
```

```{r}


# Change Kappa ------------------------------------------------------------
# Changes maximum value of kappa
# 
changemax_kappa <- function(m_kappa,
                            change){
  # Find the dimensions of the m_kappa
  n <- nrow(m_kappa)
  m <- ncol(m_kappa)
  
  # Initialize the largest element to be zero
  largest_element <- 0
  row <- NA
  col <- NA
  
  # Loop through the m_kappa and find the largest off-diagonal element
  for (i in 1:n) {
    for (j in 1:m) {
      # Skip the diagonal elements (i.e. the elements where i == j)
      if (i != j) {
        # Update the largest element if we find a larger one
        if (abs(m_kappa[i,j]) > abs(largest_element)) {
          largest_element <- m_kappa[i,j]
          row <- i
          col <- j
        }
      }
    }
  }
  print(largest_element)
  # change the element in the matrix on both sides of diagonal
  m_kappa[row,col] <- m_kappa[row,col]*change
  m_kappa[col,row] <- m_kappa[col,row]*change
  
  
  
  # Multiply the largest element by a factor and return the result
  return(m_kappa)
}

```


# Approximate distributions with density
Then, try approximating the norm distribution with known densities. This did not really lead anywhere.
```{r}
ref_dist <- l_comp_within[[1]][[3]][[6]][[5]][[1]]$res %>% 
  as.data.frame() %>% 
  filter(mat == "beta") %>% 
  filter(model_ind == 1) %>% 
  pull(null)

hist(ref_dist, breaks = 40)


cs_res<- MASS::fitdistr(ref_dist, densfun = "chi-squared", start = list(df = 1))
no_res <- MASS::fitdistr(ref_dist, densfun = "normal")

```


# Fit VAR models
```{r}
# model_a <- mgm::mvar(data = dat,
#                      type = rep("g", 6),
#                      level = rep(1,6),
#                      lambdaSel = "EBIC",
#                      lambdaGam = 0,
#                      lags = 1,
#                      consec = 1:nrow(dat),
#                      threshold = "none", 
#                      scale = TRUE)
# 
# 
# model_b <- mgm::mvar(data = dat,
#                      type = rep("g", 6),
#                      level = rep(1,6),
#                      lambdaSel = "EBIC",
#                      lambdaGam = 0,
#                      lags = 1,
#                      consec = 1:nrow(dat),
#                      threshold = "none", 
#                      scale = TRUE)
# 
# ntime_a <- length(model_a$call$data_lagged$included)
# pars_a <- model_a$wadj[,,1]
# int_a <- unlist(model_a$intercepts)
# 
# # the intercepts are so small that the data afterwards becomes weird
# # so I manually set the intercepts
# int_a <- c(-0.3, 0.1, 0.05, -0.25, 0.4, 0.01)

```



Now we simulate 100 models from the parameters of $A$. 
```{r}
# # setup parallelization
# ncores = parallel::detectCores() - 4
# cl = makeCluster(ncores)
# registerDoParallel(cl)
# 
# # somehow, the following gives me insanely weird data, basically everything is almost zero
# l_dat <- foreach(i = 1:100, .packages = "mlVAR") %dopar% {
#   mod <- simulateVAR(pars = pars_a,
#                      means = 0,
#                      Nt = ntime_a,
#                      residuals = 1)     # need to check if I should provide residual covariance matrix here
#   mod
#   
# }
# 
# 
# # fit model A to the data 100 times
# l_res_a <- foreach(i = 1:100, .packages = "mgm") %dopar% {
#   res <- predict(object = model_a, 
#                  data = l_dat[[i]])
#   errors <- res$errors
# }
# 
# # slightly change model B and fit to the data again
# for(i in 1:nrow(model_b$wadj[,,1])){
#   for(j in 1:ncol(model_b$wadj[,,1])){
#     if(model_b$wadj[,,1][i,j]>0.0001 | model_b$wadj[,,1][i,j] < -0.0001){
#       model_b$wadj[,,1][i,j] <- model_b$wadj[,,1][i,j] + rnorm (n = 1,mean = 0.05, sd = 0.01)
#     }
#   }
# }
# 
# l_res_b <- foreach(i = 1:100, .packages = "mgm") %dopar% {
#   res <- predict(object = model_b, 
#                  data = l_dat[[i]])
#   errors <- res$errors
# }
# 
# 
# stopCluster(cl)

```

This does not give a nice distribution with uncertainty.
It also gives me negative R² values, which is really weird, probably
due to the insanely small variance?
Maybe I should resample from the VAR model using the uncertainty
in $\beta$ and then compare distributions. 




## Compare error distribution to slightly different model
Add small random noise to the coefficients of the "true" model and refit to data.
This is not trivial, because the "predict" function uses posterior samples
to evaluate the prediction and not just the posterior mean. Maybe I should rewrite the function so that just the mean is used to obtain predictions.
```{r}
# slightly change model B and fit to the data again
# only change nonzero coefs
# bay_res_b <- bay_res
# for(i in 1:nrow(bay_res_b$beta_mu)){
#   for(j in 1:ncol(bay_res_b$beta_mu)){
#     if(bay_res_b$beta_mu[i,j]!=0){
#       bay_res_b$beta_mu[i,j] <- bay_res_b$beta_mu[i,j]+rnorm(1, mean = 0, sd = 0.25)
#     }
#   }
# }
# 
# ncores = parallel::detectCores() - 4
# cl = makeCluster(ncores)
# registerDoParallel(cl)
# l_refit <- foreach(i = 1:100) %dopar% {
#   source("aux_funs.R")
#   ref <- predict(object = bay_res_b,
#                  data = l_dat[[i]])
#   
# } 
# 
# 
# stopCluster(cl)
```

# Prediction approach
```{r}
# Predict function for external data --------------------------------------
# Taken from https://github.com/donaldRwilliams/BGGM/blob/master/R/predict.estimate.R
# adapted for using external data for refitting of the model
# data needs to be of the same length, which is no issue when using simulated data

predict.var_estimate <- function(object,
                                 data,
                                 summary = TRUE,
                                 cred = 0.95,
                                 iter = NULL,
                                 progress = TRUE,
                                 ...){
  
  
  # lower bound
  lb <- (1 - cred) / 2
  
  # uppder bound
  ub <- 1 - lb
  
  
  # data matrix
  # B: changed by me to accomodate external object
  # needs to be in the proper (lagged) formatting
  X <- data$X
  n <- nrow(X)
  
  
  if(is.null(iter)){
    
    iter <- object$iter
    
  }
  
  p <- object$p
  
  post_names <- sapply(1:p, function(x) paste0(
    colnames(data$Y)[x], "_",  colnames(data$X))
  )
  
  post_samps <- BGGM::posterior_samples(object)
  
  if(isTRUE(progress)){
    pb <- utils::txtProgressBar(min = 0, max = p, style = 3)
  }
  
  yhats <- lapply(1:p, function(x){
    
    yhat_p <- post_samps[, post_names[,x]] %*% t(X)
    
    
    if(isTRUE(progress)){
      utils::setTxtProgressBar(pb, x)
    }
    
    yhat_p
    
  })
  
  if(isTRUE(summary)){
    
    
    
    fitted_array <- array(0, dim = c(n, 4, p))
    
    dimnames(fitted_array)[[2]] <- c("Post.mean",
                                     "Post.sd",
                                     "Cred.lb",
                                     "Cred.ub")
    
    dimnames(fitted_array)[[3]] <- colnames(data$Y)
    
    
    for(i in 1:p){
      
      fitted_array[,,i] <- cbind(colMeans(yhats[[i]]),
                                 apply(yhats[[i]], 2, sd),
                                 apply(yhats[[i]], 2, quantile, lb),
                                 apply(yhats[[i]], 2, quantile, ub)
      )
    }
    
    
  } else {
    
    fitted_array <- array(0, dim = c(iter, n, p))
    
    dimnames(fitted_array)[[3]] <- colnames(data$Y)
    
    for(i in 1:p){
      
      fitted_array[,,i] <- t(as.matrix(yhats[[i]]))
      
    }
    
  }
  
  return(fitted_array)
  
}





# Predict with posterior mean ---------------------------------------------
# IDEA: add option to manually provide a beta matrix with posterior means
# instead of building the mean of the posterior samples

# Change the function above to only use posterior mean for prediction
predict_pmu.var_estimate <- function(object,
                                 data,
                                 summary = TRUE,
                                 cred = 0.95,
                                 iter = NULL,
                                 progress = TRUE,
                                 ...){
  
  
  # lower bound
  lb <- (1 - cred) / 2
  
  # uppder bound
  ub <- 1 - lb
  
  
  # data matrix
  # B: changed by me to accomodate external object
  # needs to be in the proper (lagged) formatting
  X <- data$X
  n <- nrow(X)
  
  
  if(is.null(iter)){
    
    iter <- object$iter
    
  }
  
  p <- object$p
  
  post_names <- sapply(1:p, function(x) paste0(
    colnames(data$Y)[x], "_",  colnames(data$X))
  )
  
  post_samps <- BGGM::posterior_samples(object)
  
  # Only use relevant Beta samples
  post_samps_b <- post_samps[,post_names]
  colnames(post_samps_b) <- post_names
  post_samps_mu <- matrix(colMeans(post_samps_b), ncol = 6)
  # Very important: Do I need to do byrow TRUE or not?
  # I don't think so, because it is done variable-wise
  # so the first 6 values are the the coefficients on V1
  
  
  if(isTRUE(progress)){
    pb <- utils::txtProgressBar(min = 0, max = p, style = 3)
  }
  
  yhats <- lapply(1:p, function(x){
    
    yhat_p <- post_samps_mu[,x] %*% t(X)
    
    
    if(isTRUE(progress)){
      utils::setTxtProgressBar(pb, x)
    }
    
    yhat_p
    
  })
  
  if(isTRUE(summary)){
    
    
    
    fitted_array <- array(0, dim = c(n, 1, p))
    
    dimnames(fitted_array)[[2]] <- c("Post.mean")
    
    dimnames(fitted_array)[[3]] <- colnames(data$Y)
    
    
    for(i in 1:p){
      
      fitted_array[,,i] <- cbind(colMeans(yhats[[i]])
      )
    }
    
    
  } else {
    
    fitted_array <- array(0, dim = c(iter, n, p))
    
    dimnames(fitted_array)[[3]] <- colnames(data$Y)
    
    for(i in 1:p){
      
      fitted_array[,,i] <- t(as.matrix(yhats[[i]]))
      
    }
    
  }
  
  return(fitted_array)
  
}

```




## Older manual way of posterior comparisons
TODO: This will need changes when we add multiple simulation conditions. 
```{r generate-rawdata}
# Storage
l_raw <- list()
# Simulation conditions
n_ind <- 100   # number of individuals (so models to create)
n_tp <- 100     # number of timepoints per time series
n_postds <- 130     # number of posterior datasets


# Think about if I actually need to standardize here
# because everything will be standardized by var_estimate anyway
ncores = parallel::detectCores() - 4
cl = makeCluster(ncores)
registerDoParallel(cl)
l_raw$graph <- lapply(l_changed_graphs$graph1, 
                      function(x) {sim_raw_parallel(
                                dgp = x, n = n_postds, 
                                tp = n_tp, means = 0,
                                standardize = TRUE)})
# l_raw$graph2 <- lapply(l_changed_graphs$graph2, 
#                       function(x) {sim_raw_parallel(
#                                 dgp = x, n = n_postds, 
#                                 tp = n_tp, means = 0,
#                                 standardize = TRUE)})
# l_raw$graph3 <- lapply(l_changed_graphs$graph3, 
#                       function(x) {sim_raw_parallel(
#                                 dgp = x, n = n_postds, 
#                                 tp = n_tp, means = 0,
#                                 standardize = TRUE)})

stopCluster(cl)
```

```{r estimate-models}
# Bayesian model parameters
rho_sd <- 0.5      # Prior for covariance matrix
beta_sd <- 1       # Prior for regression matrix
seed <- 2022
n_iter <- 5000


l_res <- list()
l_res2 <- list()
ncores = parallel::detectCores() - 2
cl = makeCluster(ncores)
registerDoParallel(cl)
l_res <- fit_var_parallel(data = l_raw$graph, n = n_ind,
                 rho_prior = rho_sd, beta_prior = beta_sd, seed = seed,
                 iterations = n_iter)
l_res2 <- fit_var_parallel(data = l_raw$graph2, n = n_ind,
                 rho_prior = rho_sd, beta_prior = beta_sd, seed = seed,
                 iterations = n_iter)
stopCluster(cl)
# write_rds(l_res, "l_res.Rds")
# l_res <- read_rds("l_res.Rds")
# write_rds(l_res2, "l_res2.Rds")
# l_res2 <- read_rds("l_res2.Rds")
```


```{r simulate-from-posterior}
## data generation first dgp
ncores = parallel::detectCores() - 2
cl = makeCluster(ncores)
registerDoParallel(cl)
l_data <- list()
l_data <- sim_from_post_parallel(fitobj = l_res, n = n_ind, n_datasets = n_postds, tp = n_tp,
                       iterations = n_iter, means = 0, convert_bggm = FALSE)
stopCluster(cl)

## second dgp
ncores = parallel::detectCores() - 2
cl = makeCluster(ncores)
registerDoParallel(cl)
l_data2 <- list()
l_data2 <- sim_from_post_parallel(fitobj = l_res2, n = n_ind, n_datasets = n_postds, tp = n_tp,
                       iterations = n_iter, means = 0, convert_bggm = FALSE)
stopCluster(cl)
# write_rds(l_data, "l_data.Rds")
# write_rds(l_data2, "l_data2.Rds")
# l_data <- read_rds("l_data.Rds")
# l_data2 <- read_rds("l_data2.Rds")

```

First, fit VAR model to posterior sample data. 
```{r fit-posterior}
# Fit the model to posterior data samples
rho_sd <- 0.5
beta_sd <- 1
seed <- 2022
n_iter <- 5000


ncores = parallel::detectCores() - 4
cl = makeCluster(ncores)
registerDoParallel(cl)
l_postresfull <- list()
before_par2 <- Sys.time()
l_postres_all <- fit_var_parallel_post(data = l_data, n = 100, nds = n_postds,
                 rho_prior = rho_sd, beta_prior = beta_sd, seed = seed,
                 iterations = n_iter, posteriorsamples = TRUE, pruneresults = TRUE) 
after_par2 <- before_par2 - Sys.time()
stopCluster(cl)
# Cut away failed attempts, always get 100 fitted results
l_postres <- lapply(l_postres_all, function(x) x[!sapply(x, is.null)])

# write_rds(l_postres, "l_postres.Rds")
# l_postres <- read_rds("l_postres.Rds")

# # Another DGP
# Use new function
ncores = parallel::detectCores() - 4
cl = makeCluster(ncores)
registerDoParallel(cl)
l_postresfull <- list()
before_par2 <- Sys.time()
l_postres2_full <- fit_var_parallel_post(data = l_data2, n = 100, nds = n_postds,
                 rho_prior = rho_sd, beta_prior = beta_sd, seed = seed,
                 iterations = n_iter, posteriorsamples = TRUE, pruneresults = TRUE) 
after_par2 <- before_par - Sys.time()
stopCluster(cl)

# Cut away failed attempts, always get 100 fitted results
l_postres2 <- lapply(l_postres2_full, function(x) x[!sapply(x, is.null)])

# write_rds(l_postres2, "l_postres2.Rds")
# l_postres2 <- read_rds("l_postres2.Rds")



```


Cross compare does not need fitpost_b and fitemp_b anymore. 
```{r same-dgp}
# Compare all possible combinations
# Need to add the different DGPs into this
comp_combinations <- expand_grid_unique(mod_a = seq(1, n_ind, 1),
                                 mod_b = seq(1, n_ind, 1))
# Number of comparison types
n_c_types <- 3

# Number of comparison combinations
n_c_comb <- nrow(comp_combinations)

comp_conditions <- data.frame(
                        postpost = rep(FALSE, n_c_comb*n_c_types),
                        mod_a = rep(comp_combinations[,1], n_c_types),
                        mod_b = rep(comp_combinations[,2], n_c_types),
                        fitpost_a = rep("l_postres", n_c_comb*n_c_types),
                        fitpost_b = rep("l_postres", n_c_comb*n_c_types),
                        fitemp_a = rep("l_res", n_c_comb*n_c_types),
                        fitemp_b = rep("l_res", n_c_comb*n_c_types),
                        n_datasets = rep(100, n_c_comb*n_c_types),
                        comparison = c(rep("frob", n_c_comb), 
                                       rep("maxdiff", n_c_comb),
                                       rep("l1", n_c_comb)))
ncores = parallel::detectCores() - 4
cl = makeCluster(ncores)
registerDoParallel(cl)
starttime <- Sys.time()
cross_compare_res <- list()
cross_compare_res <- 
  lapply(c(1:nrow(comp_conditions)), function(i){
    args <- comp_conditions[i,]
    cross_compare(
      mod_a = args$mod_a, 
      mod_b = args$mod_b,
      fitpost_a = eval(as.name(paste(args$fitpost_a))),
      fitpost_b = eval(as.name(paste(args$fitpost_b))), 
      fitemp_a = eval(as.name(paste(args$fitemp_a))), 
      fitemp_b = eval(as.name(paste(args$fitemp_b))), 
      n_datasets = args$n_datasets, 
      comparison = args$comparison
    )
    }
  )
stoptime <- Sys.time()-starttime    # ~16 seconds for 4950 comparisons!
stopCluster(cl)

# Build plot for results
eval_res <- lapply(cross_compare_res, cross_compare_eval)
df_eval_res <- as.data.frame(do.call(rbind, eval_res))
# write_rds(df_eval_res, "df_eval_res.Rds")
# df_eval_res <- read_rds("df_eval_res.Rds")

df_eval_res %>% 
  mutate(beta_a = as.numeric(beta_a),
         beta_b = as.numeric(beta_b),
         pcor_a = as.numeric(pcor_a),
         pcor_b = as.numeric(pcor_b),
         comp = as.factor(as.character(comp))) %>% 
  group_by(comp) %>% 
  # number of tries per condition
  add_count(comp) %>% 
  summarize(sig_beta = sum(beta_a < 5 & beta_b < 5)/n,
            sig_pcor = sum(pcor_a < 5 & pcor_b < 5)/n) %>% 
  ungroup() %>% 
  distinct(comp, sig_beta, sig_pcor)
plot_test_samedgp_75 <- df_eval_res %>% 
  pivot_longer(cols = c(beta_a, beta_b, pcor_a, pcor_b), names_to = "res") %>% 
  mutate(value = as.numeric(value),
         comp = as.character(comp)) %>% 
  ggplot(aes(x = value, color = comp, fill = comp))+
  geom_bar()+
  theme_minimal()+
  scale_fill_okabe_ito()+
  scale_color_okabe_ito()+
  labs(title = "Count of posterior differences larger than empirical",
       subtitle = "Simulating from same DGP (Epskamp, 2018)",
       caption = "200 Timepoints, 100 Individuals")+
  facet_wrap(~comp)+
  theme(legend.position = "none")
# ggsave("figures/plot_test_samedgp_eps200.svg", plot_test_samedgp_eps500,  device = "svg",
       # height = 12, width = 16, units = "cm")
```


Then look at different DGP. 
```{r different-dgp}
# Compare all possible combinations
comp_combinations2 <- expand_grid_unique(mod_a = seq(1, n_ind, 1),
                                 mod_b = seq(1, n_ind, 1))
comp_conditions2 <- data.frame(
                        postpost = rep(FALSE, nrow(comp_combinations2)*3),
                        mod_a = rep(comp_combinations2[,1], 3),
                        mod_b = rep(comp_combinations2[,2], 3),
                        fitpost_a = rep("l_postres", nrow(comp_combinations2)*3),
                        fitpost_b = rep("l_postres2", nrow(comp_combinations2)*3), 
                        fitemp_a = rep("l_res", nrow(comp_combinations2)*3),
                        fitemp_b = rep("l_res2", nrow(comp_combinations2)*3),
                        n_datasets = rep(100, nrow(comp_combinations2)*3),
                        comparison = c(rep("frob", nrow(comp_combinations2)), 
                                       rep("maxdiff", nrow(comp_combinations2)),
                                       rep("l1", nrow(comp_combinations2))))
ncores = parallel::detectCores() - 4
cl = makeCluster(ncores)
registerDoParallel(cl)
starttime <- Sys.time()
cross_compare_res2 <- list()
cross_compare_res2 <- 
  lapply(c(1:nrow(comp_conditions2)), function(i){
    args <- comp_conditions2[i,]
    cross_compare(
      mod_a = args$mod_a, 
      mod_b = args$mod_b,
      fitpost_a = eval(as.name(paste(args$fitpost_a))),
      fitpost_b = eval(as.name(paste(args$fitpost_b))), 
      fitemp_a = eval(as.name(paste(args$fitemp_a))), 
      fitemp_b = eval(as.name(paste(args$fitemp_b))), 
      n_datasets = args$n_datasets, 
      comparison = args$comparison
    )
    }
  )
stoptime <- Sys.time()-starttime    # ~16 seconds for 4950 comparisons!
stopCluster(cl)
# Build plot separated by PCOR and BETA and by comparison type
eval_res2 <- lapply(cross_compare_res2, cross_compare_eval)
df_eval_res2 <- as.data.frame(do.call(rbind, eval_res2))
plot_test_comp2 <- df_eval_res2 %>% 
  pivot_longer(cols = -comp, 
               names_to = c("matrix", ".value"),
               names_sep = "_") %>% 
  pivot_longer(cols = c(a,b), values_to = "value", names_to = "ref_model") %>% 
  mutate(value = as.numeric(value),
         comp = as.character(comp)) %>% 
  ggplot(aes(x = value, color = comp, fill = comp))+
  geom_histogram(bins = 100, alpha =0.8)+
  theme_minimal()+
  scale_fill_okabe_ito()+
  scale_color_okabe_ito()+
  labs(title = "Count of posterior differences larger than empirical",
       subtitle = "Simulating from different DGP (random uniform noise [-0.05, 0.05])",
       caption = "75 Timepoints, 100 Individuals")+
  facet_grid(comp~matrix)
ggsave("figures/plot_test_runifnoise75.svg", plot_test_comp2,  device = "svg",
       height = 12, width = 16, units = "cm")
```









## Older Stuff


Another idea: for each posterior sample, generate two new datasets and obtain Frobenius norm between the two regression matrices for each sample as Null distribution. 
```{r}
# takes as input one posterior sample
# generates two new datasets, re-estimates the model and then 
# computes frobenius norm between resulting estimates
# this does not work well because there is not enough error 
frob_res <- c(rep(NA, 100))
for(i in 51:151){
  frob_res[i] <- f_post_frob(l_res[[1]]$fit$beta[,,i])
}

df_frob_res <- as.data.frame(frob_res)
df_frob_res <- na.omit(df_frob_res)
df_frob_emp <- as.data.frame(frob_emp)
df_frob_emp <- na.omit(df_frob_emp)



df_frob_res %>% 
  filter(!is.na(frob_res)) %>% 
  ggplot(aes(x = frob_res))+
  geom_density()+
  geom_vline(aes(xintercept = quantile(frob_res, 0.95), col = "red"))+
  geom_point(data = df_frob_emp, aes(x = frob_emp, y = 0), col = "blue")+
  theme_minimal()


```




For later, obtain the likelihoods of each model directly and then compare these across samples. 
```{r}

```





Function for comparison of frobenius of posterior samples. 
```{r}
# Frobenius norm two datasets from each posterior sample ------------------
f_post_frob <- function(sample, seed = 2022,
                        rho_sd = 0.5, beta_sd = 1){
  set.seed = seed
  
  # Simulate
  d1 <- mlVAR::simulateVAR(pars = sample,
                           means = 0,
                           Nt = 200,
                           residuals = .1)
  d2 <- mlVAR::simulateVAR(pars = sample,
                           means = 0,
                           Nt = 200,
                           residuals = .1)
  # Estimate
  m1 <- try(BGGM::var_estimate(d1,
                               rho_sd = rho_sd,
                               beta_sd = beta_sd,
                               iter = n_iter,
                               progress = FALSE,
                               seed = seed))
  m2 <- try(BGGM::var_estimate(d2,
                               rho_sd = rho_sd,
                               beta_sd = beta_sd,
                               iter = n_iter,
                               progress = FALSE,
                               seed = seed))
  
  # Compute Frobenius Norm
  if(is.list(m1) & is.list(m2)){
    frob_norm <- norm(m1$beta_mu - m2$beta_mu, type = "F")
    return(frob_norm)
    
  }
  else return(NA)
  
  
}
```



```{r}
graphical
```







# Older Stuff

## Refitting approach
Refit all models to all 100 datasets. Use the internal function by Williams, modified by me.

Do I even need the intercept when all time series are standardized? He seems to ignore them. I also still need to understand what the 'predict' function does here and if it is even useful for me. The predict function uses the posterior samples to predict fitted values, so it even contains the foundation for the new simulated datasets? Not sure if this makes sense. Here we are however interested in the comparison of the graph that will actually be plotted, so just use the posterior mean of the beta coefficients for prediction.


This is not functioning at the moment because there is a new list layer to l_dat_bggm. 
```{r}
ncores = parallel::detectCores() - 8
cl = makeCluster(ncores)
registerDoParallel(cl)

# change i to n_ind or so
l_refit <- foreach(i = seq(2), .export = c("predict_pmu.var_estimate"))  %dopar% { # each model
  l_m_ref <- list()  # store for each model
  for(j in 1:n_postds){   # each simulated dataset
    l_m_ref[[j]] <- try(predict_pmu.var_estimate(object = l_res[[i]],
                             data = l_dat_bggm[[j]]))
  }
  return(l_m_ref)
} 



stopCluster(cl)
```

We use RMSE to evaluate the fit, but could also use Bayesian $R^2$ or similar stuff (maybe even the likelihood). As we get credible intervals for the predictions, we could also do something with structure recovery, so see which edges were included and which weren't if we opt for some kind of sparsity. 

```{r compute-rmse}
l_rmse <- list()
ncores = parallel::detectCores() - 4
cl = makeCluster(ncores)
registerDoParallel(cl)
l_rmse <- foreach(i = seq(n_ind)) %dopar% {  # for every model 
  df_rmse <- data.frame(iter = 1:n_ds,
                    rmse = rep(NA, n_ds),
                    model = rep(i, n_ds))
  for(j in 1:n_postds){      # for every dataset
    df_rmse[j,"rmse"] <- try(f_eval_refit(data = l_dat_bggm[[j]],
               refit = l_refit[[i]][[j]]))
  }
  return(df_rmse)
}

stopCluster(cl)
df_errors <- do.call(rbind, l_rmse)
# write_rds(df_errors, "df_errors.Rds")
```


Plot the error distribution:
```{r plot-error-dist}
df_errors %>% 
  mutate(rmse = as.numeric(rmse)) %>% 
  mutate(rmse = round(rmse, 3)) %>% 
  filter(model < 10) %>% 
  ggplot(aes(x = rmse, group = model , fill = as.factor(model), col = as.factor(model)))+
  ggdist::stat_halfeye(alpha = 0.7)+
  theme_minimal()+
  ggokabeito::scale_fill_okabe_ito()+
  ggokabeito::scale_color_okabe_ito()

```


Two ways of numerical comparison: First, treat as empirical distribution and use Kolmogorow-Smirnow. P-Values can be computed exactly or with Monte Carlo Sampling, I need to check which one makes the most sense. Apparently, the KS test (and similar tests) assume independence between the curves under consideration. 
```{r comp-kstest}
# Loop over test statistics
# Baseline is model 1
err1 <- df_errors$rmse[df_errors$model == 1]
df_ks_res <- data.frame(model = rep(NA, n_mod),
                        statistic = rep(NA, n_mod),
                        pval = rep(NA, n_mod))
options(scipen=999)

# Filter model that did not converge, have to do this more principled later on
df_errors <- df_errors %>% 
  mutate(rmse = as.numeric(rmse)) %>% 
  filter(!is.na(rmse))

for(m in 2:n_mod){
  if(m != 75){
  err_alt <- df_errors$rmse[df_errors$model == m]
  tmp <- stats::ks.test(x = err1,
                 y = err_alt,
                 alternative = "greater")
  df_ks_res[m,"model"] <- m
  df_ks_res[m, "statistic"] <- tmp$statistic[[1]]
  df_ks_res[m, "pval"] <- round(tmp$p.value, 5)
  }

}

# Still too many false positives?
# maybe I should actually test one-sided, because some of the models
# actually fit better than the reference model
sum(df_ks_res$pval < 0.001, na.rm = TRUE)



```

Second, estimate density, treat as empirical distribution and calculate Jenson-Shannon-Divergence (which is symmetrical, KL-Divergence is not).
The exact method for estimating the density should not matter too much, as the distribution of errors should roughly follow a normal distribution. For now, just use the density approach in the `stats` package with a Gaussian kernel. 1-JSD is a similarity metric. 
```{r}

# compute for whole dataset
jsd_res <- f_comp_jsd(df = df_errors, n = 100)

jsd_res %>% 
  ggplot(aes(x = jsd))+
  geom_density()+
  theme_minimal()


```


Another idea: compute something like absolute difference between network results, as van Borkulo et al. did it. 
```{r}

```



Functions of refitting approach: 

```{r}
# Combine y and yhat ------------------------------------------------------
# Function to get yhat and actual data into the same dataframe
# data = data object
# refit = predict.var_estimate object
f_merge_pred <- function(data, refit){
  y_pred <- array(NA, dim = c(dim(refit)[1],dim(refit)[3]))
  # loop over number of variables in refit object
  for(p in 1:dim(refit)[3]){
    y_pred[,p] <- refit[,,p]
    dimnames(y_pred) <- list(NULL, paste0("V",1:6,"_pred"))
  }
  res <- cbind(data$Y, y_pred)
  return(res)
}




# Compute RMSE ------------------------------------------------------------
# everything should be standardized
f_rmse <- function(res){
  # loop through the merged results
  # 1st column with 7th, 2nd column with 8th etc.
  ncol <- dim(res)[2]/2
  res_rmse <- array(NA, dim = c(1, ncol))
  for(j in 1:ncol){
    res_rmse[,j] <- sqrt(sum((res[,j]-res[,j+ncol])^2))
  }
  colnames(res_rmse)[1:ncol] <- paste0(colnames(res)[1:ncol],"_rmse")
  return(res_rmse)
}





# Evaluate refit ----------------------------------------------------------
# function that combines everything
f_eval_refit <- function(data,
                         refit){
  comb <- f_merge_pred(data = data, refit = refit)
  fit_stat <- f_rmse(comb)
  mean_rmse <- rowMeans(fit_stat)
  
}



# JSD between reference and other error distributions ---------------------
# ref_model = number of model for reference
# n = number of generated datasets
f_comp_jsd <- function(df = df_errors, ref_model = 1, n){
  # create storage
  l_err <- list()
  l_ecdf <- list()
  df_jsd <- data.frame(model = rep(NA, n),
                       jsd = rep(NA, n))
  
  
  # Obtain characteristics of reference model
  # obtain RMSEs
  tmp <- subset(df_errors, model == ref_model, select = rmse)
  rmse_refmod <- tmp$rmse
  
  # obtain ECDFs
  f_ecdf_ref <- stats::ecdf(rmse_refmod)
  ecdf_refmod <- f_ecdf_ref(rmse_refmod)
  
  # setup loop
  for(i in seq(n)){
    # if model has errors stored
    if(nrow(subset(df_errors, model == i, select = rmse)) > 0){
      # obtain RMSEs
      tmp <- subset(df_errors, model == i, select = rmse)
      rmse_mod <- tmp$rmse
      
      # obtain ECDFs
      f_ecdf <- stats::ecdf(rmse_mod)
      ecdf_mod <- f_ecdf(rmse_mod)
      
      # compute JSD to reference distribution
      v_ecdf <- rbind(ecdf_refmod, ecdf_mod)
      jsd <- philentropy::JSD(v_ecdf)
      
      # store values
      df_jsd[i,"model"] <- i
      df_jsd[i, "jsd"] <- jsd
    }

   
  }
  
  return(df_jsd)
}


# Compute likelihood ------------------------------------------------------
# https://github.com/cran/graphicalVAR/blob/master/R/graphicalVAR.R
# starting with line 158
# kappa = precision matrix of residuals
# sigma = unconstrained covariance matrix of residuals

logll <- function(kappa, sigma, n){
  lik1 <- determinant(kappa)$modulus[1]
  lik2 <- sum(diag(kappa %*% sigma))
  
  llk <- (n/2)*(lik1-lik2)
  llk
}

```
















Just a quick check on how well parameters are recovered: 
```{r}
test <- list()
for(i in 1:length(l_res)){
  test[[i]] <- l_res[[i]]$beta_mu
}
a_test <- array(unlist(test), c(6,6,100))

# means across refit samples
rowMeans(a_test, dims = 2)
# comparison with ground truth
t(graph$beta)
```









```{r}
# Posterior samples covariance matrix ------------------------------------
# res = results of var_estimate
f_postcov <- function(res){
  beta_posterior <- res$fit$beta
  # delete warm-up samples
  beta_posterior <- beta_posterior[,,51:5050]

  # get mean and SD of posterior estimates
  beta_mu <- round(apply(beta_posterior,1:2,mean), digits = 3)
  beta_sd <- round(apply(beta_posterior,1:2,sd), digits = 3)

  # obtain the covariance matrix of estimates
  dimnames(beta_posterior)[[1]] <- c("V1.l1", "V2.l1", "V3.l1", "V4.l1", "V5.l1", "V6.l1")
  dimnames(beta_posterior)[[2]] <- c("V1", "V2", "V3", "V4", "V5", "V6")

  # convert array to list
  l_beta_posterior <- lapply(seq(dim(beta_posterior)[3]), function(x) beta_posterior[,,x])

  ldf_beta_posterior <- lapply(l_beta_posterior, function(x){reshape2::melt(as.matrix(x))})

  # keep sample index
  df_beta_posterior <- purrr::map_dfr(ldf_beta_posterior, .f = rbind, .id = "index")

  # pivot wider to obtain cov-matrix of predictors across posterior samples
  vcov_beta <- df_beta_posterior |>
    tidyr::pivot_wider(id_cols = index,
                names_from = c(Var1, Var2)) |>
    dplyr::select(!index) |>
    stats::cov()
  return(vcov_beta)
}


# Get Beta Variance -------------------------------------------------------
# Input: fit object res
# iter: number of iterations
f_betavar <- function(res){
  iter <- res$iter
  beta_var <- apply(res$fit$beta[,,51:(res$iter+50)], 1:2, var)   # delete first 50 samples
  beta_var
  
}
  
  


# Compute reliability -----------------------------------------------------
# Input: Vector of beta weights and posterior samples covariance matrix
f_rel <- function(beta, covmat){
  var_bw <- sd(beta)
  var_wi <- tr(covmat)
}



```








```{r}

# Plot error distribution -------------------------------------------------
plot_error_dist <- function(dat, errorcol = mse){
  ggplot(dat, aes(x = {{errorcol}}))+
  ggdist::stat_dist_halfeye(fill = ggokabeito::palette_okabe_ito()[2],
                            color = "black")+
  theme_minimal()
}


```





# Within Compare

```{r}
## Looping is deprecated
# # as many clusters as n_g_types
# cl = makeCluster(n_g_types)
# registerDoParallel(cl)
# # Loop over data-generating processes
# for(d in seq(n_dgp)){
#   l_comp_within[[d]] <- list()
#   # Loop over timepoint conditions
#   for(t in seq(n_t_types)){
#     l_comp_within[[d]][[t]] <- list() 
#     # Loop over changed graphs
#     for(c in seq(n_g_types)){
#       l_comp_within[[d]][[t]][[c]] <- list()
#       # Loop over model combinations
#       for(m in seq(n_c_comb)){
#         l_comp_within[[d]][[t]][[c]][[m]] <- list()
#         # Loop over comparison types
#         for(c_type in seq(n_c_types)){
#           l_comp_within[[d]][[t]][[c]][[m]][[c_type]] <- list()
#           l_comp_within[[d]][[t]][[c]][[m]][[c_type]]$res <- within_compare(
#                       mod_a = comp_combinations[m, 1],
#                       mod_b = comp_combinations[m, 2],
#                       # [[1]] indexes the truegraph
#                       fitpost_a = l_res[[d]][[t]][[1]],
#                       fitpost_b = l_res[[d]][[t]][[c]],
#                       fitemp_a = l_res[[d]][[t]][[1]],
#                       fitemp_b = l_res[[d]][[t]][[c]],
#                       n_datasets = 100,
#                       n_draws = 1000,
#                       comparison = comp_names[c_type],
#                       postpred = FALSE)
#          # Store relevant parameters for evaluation
#          l_comp_within[[d]][[t]][[c]][[m]][[c_type]]$params <- list(
#            dgp = dgp_names[d],
#            tp = n_tp[t],
#            comp_graph = change_names[c]
#          )
#         } 
# 
#         
#         
#       }
#     }
#   }
# }
# 
# # write_rds(l_comp_within, "l_comp_within.RDS")

```






Visualize reference distributions and empirical value
```{r}
pdf(file = "figures/distributions/reference_dists_xlim10.pdf")
for(i in 1:4){
  for(j in 1:7){
    for(c in 1:3){
          b <- plot_test(l_comp_within[[1]][[i]][[j]][[5]][[c]], modmat = beta)
          plot(b)
          p <- plot_test(l_comp_within[[1]][[i]][[j]][[5]][[c]], modmat = pcor)
          plot(p)      
      }
  }
}
dev.off()


```

Then, try approximating the norm distribution with known densities. This did not really lead anywhere.
```{r}
ref_dist <- l_comp_within[[1]][[3]][[6]][[5]][[1]]$res %>% 
  as.data.frame() %>% 
  filter(mat == "beta") %>% 
  filter(model_ind == 1) %>% 
  pull(null)

hist(ref_dist, breaks = 40)


cs_res<- MASS::fitdistr(ref_dist, densfun = "chi-squared", start = list(df = 1))
no_res <- MASS::fitdistr(ref_dist, densfun = "normal")

```




# Old functions
```{r}
compare_var_old <- function(fit_a, 
                        fit_b, 
                        cutoff = 5,           # percentage level of test
                        dec_rule = "OR",
                        n_draws = 1000,
                        comp = "frob",
                        return_all = FALSE){  # return all distributions?
  
  require(magrittr)
  
  ## Helper function for computing distance metrics
  compute_metric <- function(a, b, metric) {
    tryCatch({
      if (metric == "frob") {
        norm(a - b, type = "F")
      } else if (metric == "maxdiff") {
        max(abs(a - b))
      } else if (metric == "l1") {
        sum(abs(a - b))
      }
    }, error = function(e) NA)
  }
  
  ## Create reference distributions for both models
  ref_a <- post_distance_within(fit_a, comp = comp, pred = FALSE, draws = n_draws)
  ref_b <- post_distance_within(fit_b, comp = comp, pred = FALSE, draws = n_draws)
  
  ## Empirical distance
  # Compute empirical distance as test statistic
  if(comp == "frob"){
    normtype = "F"
    # Compute Distance of empirical betas between a and b
    emp_beta <- tryCatch(norm(fit_a$beta_mu - fit_b$beta_mu, type = normtype), error = function(e) {NA})

    # Compute Distance of empirical pcors between a and b
    emp_pcor <- tryCatch(norm(fit_a$pcor_mu - fit_b$pcor_mu, type = normtype), error = function(e) {NA})

  }

  if(comp == "maxdiff"){
    # Compute maxdiff of empirical betas between a and b
    emp_beta <- tryCatch(max(abs(fit_a$beta_mu - fit_b$beta_mu)), error = function(e) {NA})

    # Compute maxdiff of empirical pcors between a and b
    emp_pcor <- tryCatch(max(abs(fit_a$pcor_mu - fit_b$pcor_mu)), error = function(e) {NA})

  }

  if(comp == "l1"){
    # Compute l1 of empirical betas between a and b
    emp_beta <- tryCatch(sum(abs(fit_a$beta_mu - fit_b$beta_mu)), error = function(e) {NA})

    # Compute l1 of empirical pcors between a and b
    emp_pcor <- tryCatch(sum(abs(fit_a$pcor_mu - fit_b$pcor_mu)), error = function(e) {NA})

  }
  emp_beta <- compute_metric(fit_a$beta_mu, fit_b$beta_mu, comp)
  emp_pcor <- compute_metric(fit_a$pcor_mu, fit_b$pcor_mu, comp)
  
  
  ## Combine results
  res_beta <- data.frame(null = c(unlist(ref_a[["beta"]]), unlist(ref_b[["beta"]])),
                         mod = c(rep("mod_a", n_draws), rep("mod_b", n_draws)),
                         emp = rep(emp_beta, n_draws*2),
                         comp = rep(comp, n_draws*2))
  
  
  res_pcor <- data.frame(null = c(unlist(ref_a[["pcor"]]), unlist(ref_b[["pcor"]])),
                         mod = c(rep("mod_a", n_draws), rep("mod_b", n_draws)),
                         emp = rep(emp_pcor, n_draws*2),
                         comp = rep(comp, n_draws*2))
  
  ## Implement decision rule "OR"
  if(dec_rule == "OR"){
    suppressWarnings(sig_beta <- res_beta %>% 
                       dplyr::group_by(mod) %>% 
                       dplyr::summarize(sum_larger = sum(null > emp)) %>% 
                       dplyr::summarize(sig = ifelse(sum_larger < cutoff * (n_draws/100), 1, 0)) %>% 
                       dplyr::summarize(sig_decision = sum(sig)) %>% 
                       dplyr::pull(sig_decision))
    
    suppressWarnings(larger_beta <- res_beta %>% 
                       dplyr::group_by(mod) %>% 
                       dplyr::summarize(sum_larger = sum(null > emp))) %>% 
      dplyr::pull(sum_larger)
    
    suppressWarnings(sig_pcor <- res_pcor %>% 
                       dplyr::group_by(mod) %>% 
                       dplyr::summarize(sum_larger = sum(null > emp)) %>% 
                       dplyr::summarize(sig = ifelse(sum_larger < cutoff * (n_draws/100), 1, 0)) %>% 
                       dplyr::summarize(sig_decision = sum(sig)) %>% 
                       dplyr::pull(sig_decision))
    
    suppressWarnings(larger_pcor<- res_pcor %>% 
                       dplyr::group_by(mod) %>% 
                       dplyr::summarize(sum_larger = sum(null > emp))) %>% 
      dplyr::pull(sum_larger)
    
    
  }
  
  # sig_beta <- as.numeric(sig_beta)
  # larger_beta <- as.numeric(larger_beta)
  # sig_pcor <- as.numeric(sig_pcor)
  # larger_pcor <- as.numeric(larger_pcor)
  
  
  if(!return_all){
    l_res <- list(sig_beta = sig_beta,
                  sig_pcor = sig_pcor,
                  # res_beta = res_beta,
                  # res_pcor = res_pcor,
                  emp_beta = emp_beta,
                  emp_pcor = emp_pcor,
                  larger_beta = larger_beta,
                  larger_pcor = larger_pcor)
    
  }
  if(isTRUE(return_all)){
    l_res <- list(sig_beta = sig_beta,
                  sig_pcor = sig_pcor,
                  res_beta = res_beta,
                  res_pcor = res_pcor,
                  emp_beta = emp_beta,
                  emp_pcor = emp_pcor,
                  larger_beta = larger_beta,
                  larger_pcor = larger_pcor)
    
  }
  
  
  
  return(l_res)
  
  
  
}
```



# Old empirical example



##### OLD !!!!!!!!!!!!!!!!!!!!!!

# Preparation and Data Cleaning
```{r}
# data <- readRDS("data_Fisher2017.RDS")
# 
# 
# # Number of observations per model
# n_obs <- data %>% 
#   count(subj_id)
# 
# 
# 
# # Look at proportion of missingness
# data %>% 
#   dplyr::mutate(miss = ifelse(is.na(energetic), 1, 0)) %>% 
#   dplyr::add_count(subj_id) %>% 
#   dplyr::group_by(subj_id) %>% 
#   dplyr::mutate(sum_miss = sum(miss)) %>% 
#   dplyr::summarize(prop_miss = sum_miss/n) %>% 
#   dplyr::distinct(subj_id, prop_miss) %>% 
#   dplyr::arrange(desc(prop_miss))

```


Create histograms for each individual
```{r}
l_hist <- list()
for(u in unique(data$subj_id)){
  tmp_dat <- subset(data, subj_id == u)
  l_hist[[u]] <- Hmisc::hist.data.frame(tmp_dat[,2:10])
  
}

# Create overall histograms
# Hmisc::hist.data.frame(data[,2:23])

```

Focus on some items with relatively adequate distributions. 
Impute missing data.
```{r}
# Relevant variables
rel_vars <- c("content", "fatigue", "concentrate", "positive", "hopeless", "enthusiastic")

cut_data <- data %>% 
  # dplyr::filter(subj_id %in% rel_ids) %>% 
  dplyr::select(c(all_of(rel_vars), subj_id)) %>% 
  group_by(subj_id) %>% 
  mutate(tp = row_number()) %>% 
  ungroup()

# impute with Kalman filter

imp_data <- cut_data %>% 
  group_by(subj_id) %>% 
  mutate(across(all_of(rel_vars),  # columns to be imputed
                ~imputeTS::na_kalman(.x, type = "level"))) %>%        
                # .names = "{.col}_imp")) %>%                      # add "_imp" to new column names
  ungroup()

# THIS IS NOW DONE LATER!
# short_data <- imp_data %>% 
#   split(.$subj_id)

```



## Detrending
Remove linear trend
```{r}
data_detrend <- lapply(short_data, function(x){
  for (v in 1:length(rel_vars)){
  # Regress on time
  lm_form <- as.formula(paste0(rel_vars[v], "~ tp"))
  lm_res <- summary(lm(lm_form, data = x))
  # detrend with residuals
  # [,4] accesses p-values
  # [2] p-value of beta of tp
  if(lm_res$coefficients[,4][2] < 0.05){
    print(paste0("Detrending variable: ", rel_vars[v]))
    x[!is.na(x[rel_vars[v]]),rel_vars[v]] <- residuals(lm_res)
  }
  
}
return(x)
})

```



# Fitting
Fit all models
```{r}
rho_prior = 0.25
beta_prior = .5
iterations = 50000
seed = 2022

df_prior <- data.frame(
  rho_prior = c(0.1, 0.25, 0.5),
  beta_prior = c(0.25, 0.5, 1)
)


l_fits <- list()

# Delete id and tp column for fitting
data_detrend <- lapply(data_detrend, function(x){
  x <- subset(x, select=-c(subj_id,tp))
  x
})

cl = makeCluster(3)
registerDoParallel(cl)
l_fits <- foreach(i = 1:nrow(df_prior), .packages = "BGGM") %dopar% {
  r_p <- df_prior[i,"rho_prior"]
  b_p <- df_prior[i, "beta_prior"]
  
  fit <- lapply(data_detrend, function(x){
  tryCatch(BGGM::var_estimate(Y = x,      
                     rho_sd = r_p,
                     beta_sd = b_p,
                     iter = 50000,
                     seed = 2022), error = function(e) NA)

    
    
})

  return(fit)
}

stopCluster(cl)  
  
l_fits_c <- list()
for(i in 1:3){
  l_fits_c[[i]] <- lapply(l_fits[[i]], function(x){
  x$fit$Sigma <- NULL
  x$fit$fisher_z <- NULL
  x
})
  
}

# saveRDS(l_fits_c, here::here("data/empirical_example/l_fits_empirical_ex.RDS"))
# l_fits_c <- readRDS(here::here("data/empirical_example/l_fits_empirical_ex.RDS"))


```





# Plot Distances visualized as Network

Extract distances: 
```{r}
df_emp <- data.frame(emp_beta = rep(NA, 1600),
                     emp_pcor = rep(NA, 1600))
for(n in 1:length(l_comps)){
  df_emp[n,1] <- l_comps[[n]]$emp_beta
  df_emp[n,2] <- l_comps[[n]]$emp_pcor
}
# Combine with model information
df_emp <- cbind(df_emp, fit_grid)

# Filter irrelevant comparisons
df_emp <- df_emp %>% 
  filter(fit_a != fit_b)


# Correct data structure: 40 by 40 matrix
dist_beta_emp <- df_emp %>% 
  dplyr::select(emp_beta, fit_a, fit_b) %>% 
  reshape2::acast(., fit_a ~ fit_b, value.var = "emp_beta")


dist_pcor_emp <- df_emp %>% 
  dplyr::select(emp_pcor, fit_a, fit_b) %>% 
  reshape2::acast(., fit_a ~ fit_b, value.var = "emp_pcor")


```


Now visualize with qgraph
```{r}
# The shorter the distance, the larger the connection should be!
# Convert distance to similarity matrix
simil_beta_emp <- 1/mat_beta_emp
simil_pcor_emp <- 1/mat_pcor_emp

svg("emp_example_similarity_net.svg", width = 15, height = 12)
par(mfrow = c(1,2))
qgraph(simil_beta_emp, layout = "spring", title = "Beta")
qgraph(simil_pcor_emp, layout = "spring", title = "PCOR")
dev.off()


```


Also visualize with heatmap
```{r}
# Levels
id_lvls <- as.character(1:40)

# Get min and max similarities
min(simil_beta_emp, na.rm = TRUE)
min(simil_pcor_emp, na.rm = TRUE)
max(simil_beta_emp, na.rm = TRUE)
max(simil_pcor_emp, na.rm = TRUE)



p1 <- simil_beta_emp %>% 
  as.data.frame() %>% 
  rownames_to_column("id") %>% 
  pivot_longer(-c(id), names_to = "id2", values_to = "sim") %>% 
  mutate(id = fct_relevel(id, id_lvls),
         id2 = fct_relevel(id2, id_lvls)) %>% 
  ggplot(aes(x = id, y = id2, fill = sim))+
  geom_raster()+
  scale_fill_viridis_c(limits = c(0.1, 0.6))+
  labs(x = "ID",
       y = "ID",
       title = "Similarity Matrix Beta Weights",
       caption = "Frobenius Norm",
       fill = "Similarity")


p2 <- simil_pcor_emp %>% 
  as.data.frame() %>% 
  rownames_to_column("id") %>% 
  pivot_longer(-c(id), names_to = "id2", values_to = "sim") %>% 
  mutate(id = fct_relevel(id, id_lvls),
         id2 = fct_relevel(id2, id_lvls)) %>% 
  ggplot(aes(x = id, y = id2, fill = sim))+
  geom_raster()+
  scale_fill_viridis_c(limits = c(0.1, 0.6))+
  labs(x = "ID",
       y = "ID",
       title = "Similarity Matrix pcor Weights",
       caption = "Frobenius Norm",
       fill = "Similarity")

p <- cowplot::plot_grid(p1, p2)
ggsave("similarity_matrix_empirical_example.svg", p, device = "svg", path = here("figures/"), width = 15)



```


