---
title: "var-compare-empirical-example"
author: "Bj√∂rn Siepe"
date: "2023-01-30"
output: html_document
---


```{r}
library(BGGM)
library(qgraph)
library(tidyverse)
library(foreach)
library(doParallel)
library(parallel)
library(Hmisc)
library(imputeTS)
source("aux_funs.R")
```


# Example prior distributions
Draw 5000 observations for temporal and contemporaneous priors. 
```{r}
## Temporal
# Simply sample from normal distribution
wide_beta <- rnorm(n = 5000, mean = 0, sd = 1)
narrow_beta <- rnorm(n = 5000, mean = 0, sd = 0.5)

## Contemporaneous
# Wide prior
wide_pcor <- BGGM::plot_prior(prior_sd = 0.5, iter = 5000)
wide_pcor <- wide_pcor$plot_env$prior_samp$pcors[1,2,]

# Narrow prior
narrow_pcor <- BGGM::plot_prior(prior_sd = 0.25, iter = 5000)
narrow_pcor <- narrow_pcor$plot_env$prior_samp$pcors[1,2,]


df_prior <- data.frame(
  value = c(wide_beta, narrow_beta, wide_pcor, narrow_pcor),
  mat = c(rep("Temporal",2*5000), rep("Contemporaneous", 2*5000)),
  prior = rep(c(rep("wide", 5000), rep("narrow", 5000)),2),
  label = c(rep("1", 5000), rep("0.5", 5000), rep("0.5", 5000), rep("0.25", 5000))
)


# Create grid plot
prior_example <- df_prior %>% 
  mutate(mat = as.factor(mat),
         prior = as.factor(prior)) %>% 
  ggplot(aes(x = value))+
  geom_histogram(fill = ggokabeito::palette_okabe_ito()[2],
                 col = "black",
                 bins = 50)+
  theme_compare()+
  ggh4x::facet_nested(prior~mat, scales = "free_x",)+
  labs(y = "",
       x = "")+
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        strip.text = element_text(size = 20))

prior_example
ggsave("prior_example.svg", prior_example, path = here::here("figures/") ,device = "svg", width = 10, height = 7)


```



# Empirical Example
We use data from Fisher et al. (2017), downloaded from https://github.com/jmbh/EmotionTimeSeries/blob/master/DataClean/Fisher2017/data_Fisher2017.RDS. 


## NEW Loading of data via loop.
Data were now downloaded from OSF.
Functions needed:
```{r}
lagpad <- function(x, k) {
     c(rep(NA, k), x)[1 : length(x)] 
 }
```


Then use code adapted from Fisher et al. https://osf.io/m63ks to perform cubic spline interpolation to resample to even time intervals for observations. 
```{r}
file_list <- list.files(path = here::here("data/empirical_example/fisher_2017"), pattern = ".RData", full.names = TRUE)

# Number of items
n_items <- 21
rel_items <-  c("energetic", "enthusiastic", "content", "irritable", "restless", "worried",  "guilty",  "afraid", "anhedonia", "angry", "hopeless", "down", 
"positive", "fatigue", "tension", "concentrate", "ruminate", "avoid_act", "reassure",  "procrast", "avoid_people")

l_data <- list()
for(i in 1:length(file_list)){
  l_data[[i]] <- list()
  load(file_list[[i]])
  
  # # correct names
  # colnames(data) <- c("start","finish","energetic","enthusiastic","content","irritable","restless","worried","guilty","afraid","anhedonia","angry","hopeless","down","positive","fatigue","tension","concentrate","accepted","threatened","ruminate","avoid_act","reassure","procrast","hours","difficult","unsatisfy","avoid_people")
  
  # add lags
  data$lag=lagpad(data$start,1)

  # Calculate time differences
  data$tdif=as.numeric(difftime(strptime(data$start,"%m/%d/%Y %H:%M"),strptime(data$lag,"%m/%d/%Y %H:%M")))

  # Replace NA
  data$tdif[is.na(data$tdif)] <- 0
  
  # give names to columns with missing names
  # Get the index of NA column names
  na_cols <- which(is.na(colnames(data)))

  # Replace NA column names with new names
  colnames(data)[na_cols] <- paste0("new_name_", na_cols)
  
  # data <- data %>% 
  #   dplyr::mutate(across(all_of(rel_items),  # columns to be imputed
  #               ~imputeTS::na_kalman(.x, type = "level")))
  # l_data[[i]]$test <- data

  # Calculate cumulative sum of numeric elapsed time
  data$cumsumT = cumsum(data$tdif)

  # Subset data
  trim=data[,c(3:18,21:24,28)]
  dedat=data.frame(matrix(ncol = dim(trim)[2], nrow = dim(trim[1])))
  colnames(dedat) <- colnames(trim)

  ### Detrend
  for(d in 1:n_items){
    dedat[,d] = resid(lm(scale(trim[,d])~data$cumsumT, na.action = na.exclude))
  }
  ### Cubic spline interpolation
  datcub <- data.frame(matrix(ncol=21,nrow=nrow(data)))
  for(d in 1:n_items){
    datcub[,d]=(spline(x = data$cumsumT, y=dedat[,d], nrow(data), method='fmm'))$y
  }
  colnames(datcub) <- colnames(dedat)
  
  # Only choose relevant data for us
  datcub <- datcub %>% 
    dplyr::select(c("content", "fatigue", "concentrate", "positive", "hopeless", "enthusiastic"))

  # l_data[[i]]$raw_data <- data
  # l_data[[i]]$data <- datcub
  l_data[[i]] <- datcub
  
}
# remove everything but the data list
rm(list=setdiff(ls(), "l_data"))

```


# Fit new models
```{r}
rho_prior = 0.25
beta_prior = .5
iterations = 50000
seed = 2022

df_prior <- data.frame(
  rho_prior = c(0.1, 0.25, 0.5),
  beta_prior = c(0.25, 0.5, 1)
)


l_fits <- list()


cl = makeCluster(3)
registerDoParallel(cl)
l_fits <- foreach(i = 1:nrow(df_prior), .packages = "BGGM") %dopar% {
  r_p <- df_prior[i,"rho_prior"]
  b_p <- df_prior[i, "beta_prior"]
  
  fit <- lapply(l_data, function(x){
  tryCatch(BGGM::var_estimate(Y = x,      
                     rho_sd = r_p,
                     beta_sd = b_p,
                     iter = 50000,
                     seed = 2022), error = function(e) NA)

    
    
})

  return(fit)
}

stopCluster(cl)  
  
l_fits_c <- list()
for(i in 1:3){
  l_fits_c[[i]] <- lapply(l_fits[[i]], function(x){
  x$fit$Sigma <- NULL
  x$fit$fisher_z <- NULL
  x
})
  
}
```



## Convergence 

```{r}
# # Trace Plots
# BGGM::convergence(l_fits[[1]], type = "trace",  param = "reassure--accepted")
# 
# # ACF
# BGGM::convergence(l_fits[[1]], type = "acf", param = "down--enthusiastic")
# 
# 
# # pdf("convergence_empirical.pdf")
# # for(i in 1:10){
# #   for(n in param_names){
# # 
# #   print(BGGM::convergence(l_fits[[1]], type = "acf", param = n))
# #   
# # }
# # }
# # dev.off()
# 
# # Try to calculate effective sample size
# bet <- l_fits[[1]]$fit$beta[,,51:iterations+50]
# 
# # Convert to matrix where each col is a variable
# m_bet <- t(matrix(bet, 6*6, iterations))
# mcmc_bet <- as.mcmc(m_bet)
# coda::effectiveSize(mcmc_bet)
# 
# # Same for pcor
# pcor <- l_fits[[1]]$fit$pcors[,,51:iterations+50]
# m_pcor <- t(matrix(pcor, 6*6, iterations))
# mcmc_pcor <- as.mcmc(m_pcor)
# ces <- coda::effectiveSize(mcmc_pcor)
# 
# 
# # Get ESS for all samples
# l_ess <- lapply(l_fits, var_ess)

```

# Test
Test across all participants and across different priors. I ran into some memory issues here, so I splitted based on priors. 
```{r}
l_fits <- l_fits_c
fit_grid <- expand.grid(fit_a = 1:length(l_fits[[1]]), 
                        fit_b = 1:length(l_fits[[1]]), 
                        priors = seq(nrow(df_prior), 1), 
                        comp = c("l1", "frob", "maxdiff"))
l_comps <- list()

cl = makeCluster(14)
registerDoParallel(cl)
clusterExport(cl, c("compare_var", "post_distance_within"))
l_fits_1 <- l_fits[[1]]
fit_grid_1 <- fit_grid %>% 
  filter(priors == 1)
l_comps_1 <- foreach(i = 1:nrow(fit_grid_1)) %dopar% {
  
  # Extract information
  a <- fit_grid_1$fit_a[i]
  b <- fit_grid_1$fit_b[i]
  p_ind <- fit_grid_1$priors[i]
  compa <-  as.character(fit_grid_1$comp[i])


  
  if(is.list(l_fits_1[[a]]) & is.list(l_fits_1[[b]])){

  comp_res  <- compare_var(l_fits_1[[a]],l_fits_1[[b]], comp = compa)

  return(comp_res)
  }
  else
    return(NA)
  
}



l_fits_2 <- l_fits_c[[2]]
fit_grid_2 <- fit_grid %>% 
  filter(priors == 2)
l_comps_2 <- foreach(i = 1:nrow(fit_grid_2)) %dopar% {
  
  # Extract information
  a <- fit_grid_2$fit_a[i]
  b <- fit_grid_2$fit_b[i]
  p_ind <- fit_grid_2$priors[i]
  compa <-  as.character(fit_grid_2$comp[i])

  
  if(is.list(l_fits_2[[a]]) & is.list(l_fits_2[[b]])){
  comp_res  <- compare_var(l_fits_2[[a]],l_fits_2[[b]], comp = compa)

  return(comp_res)
  }
  else
    return(NA)
  
  
    
  
  
}

rm(l_fits_2)



l_fits_3 <- l_fits_c[[3]]
fit_grid_3 <- fit_grid %>% 
  filter(priors == 3)
l_comps_3 <- foreach(i = 1:nrow(fit_grid_3)) %dopar% {
  
  # Extract information
  a <- fit_grid_3$fit_a[i]
  b <- fit_grid_3$fit_b[i]
  p_ind <- fit_grid_3$priors[i]
  compa <-  as.character(fit_grid_3$comp[i])

  
  if(is.list(l_fits_3[[a]]) & is.list(l_fits_3[[b]])){

  comp_res  <- compare_var(l_fits_3[[a]],l_fits_3[[b]], comp = compa)

  return(comp_res)
  }
  else
    return(NA)
  
}
stopCluster(cl)
rm(l_fits_3)

df_comps2_1 <- do.call(rbind, l_comps_1)
df_comps2_2 <- do.call(rbind, l_comps_2)
df_comps2_3 <- do.call(rbind, l_comps_3)

# Add prior information
df_comps2_1c <- df_comps2_1 %>% 
  cbind(fit_grid_1)
df_comps2_2c <- df_comps2_2 %>% 
  cbind(fit_grid_2)
df_comps2_3c <- df_comps2_3 %>% 
  cbind(fit_grid_3)



df_compsc <- as.data.frame(rbind(df_comps2_1c, df_comps2_2c, df_comps2_3c))

# delete unnecessary columns
df_comps <- df_compsc %>% 
  dplyr::select(-c(larger_beta, larger_pcor)) %>% 
  as.data.frame()

saveRDS(df_comps, file = here::here("data/empirical_example/df_comps_cubicspline.RDS"))

```


## Prior Sensitivity Plot
```{r}
# Calculate amount of possible combinations per condition
# 1560
# we count all comparisons twice, which is useless, but also does not change results
df_comps %>% 
  filter(fit_a != fit_b) %>% 
  group_by(priors, comp) %>% 
  count()


prior_names <- c(
  "1" = "Rho: 0.1, Beta: 0.25",
  "2" = "Rho: 0.25, Beta: 0.5",
  "3" = "Rho: 0.5, Beta: 1"
)


emp_ex_prior_sensitivity <- df_comps %>% 
  dplyr::filter(fit_a != fit_b) %>% 
  dplyr::mutate(across(c(contains("beta"), contains("pcor")),
                ~as.numeric(.))) %>% 
  dplyr::group_by(priors, comp) %>% 
  dplyr::summarize(sum_sig_beta = sum(ifelse(sig_beta >0, 1, 0)),
            sum_sig_pcor = sum(ifelse(sig_pcor >0, 1, 0)),
            mean_emp_beta = mean(emp_beta, na.rm =  TRUE)) %>% 
  dplyr::mutate(prop_sig_beta = sum_sig_beta / 1560,
         prop_sig_pcor = sum_sig_pcor / 1560) %>% 
  pivot_longer(cols = c("sum_sig_beta", "sum_sig_pcor"), names_to = "mat_sum", values_to = "sum") %>% 
  pivot_longer(cols = c("prop_sig_beta", "prop_sig_pcor"), names_to = "mat_prop", values_to = "prop") %>% 
  ggplot(aes(y = comp, x = prop, group = mat_prop, fill = mat_prop))+
  geom_bar(position = "dodge", stat = "identity")+
  # geom_point()+
  # geom_segment(aes(x = prop, xend = prop, y = 0, yend = comp))+
  coord_flip()+
  facet_wrap(priors~., labeller = as_labeller(prior_names))+
  theme_minimal()+
  ggokabeito::scale_fill_okabe_ito(labels = c("Temporal", "Contemporaneous"))+
  scale_x_continuous(limits = c(0,1),labels = function(x) paste0(x*100, "%"))+
  labs(x = "Norm",
       y = "Proportion Significant",
       title = "Prior Sensitivity Empirical Example",
       fill = "Matrix")
emp_ex_prior_sensitivity

ggsave("emp_ex_prior_sensitivity_cubicspline.svg", emp_ex_prior_sensitivity, device = "svg", path = here::here("figures/"), width = 7, height = 5)

```





## Plot all networks

```{r}
plot_net <- function(fit_obj){
  par(mfrow=c(1,2))
  qgraph::qgraph(fit_obj$beta_mu, title = "Temporal", theme = "colorblind", layout = "circle")
  qgraph::qgraph(fit_obj$pcor_mu, title = "Contemporaneous", theme = "colorblind", layout = "circle")
}


```








### Find significant and nonsignificant
Extract interesting cases
```{r}
# Find interesting cases with similar amount of observations
# nonsignificant 
# L1
# for rho 0.5, beta 1: 304
df_comps %>% 
  filter(sig_beta == 0 & sig_pcor  == 0) %>% 
  filter(fit_a != fit_b) %>% 
  left_join(n_obs, by = join_by(fit_a == subj_id)) %>% 
  left_join(n_obs, by = join_by(fit_b == subj_id), suffix = c("_a", "_b")) %>% 
  mutate(diff_n = abs(n_a - n_b)) %>% 
  arrange(diff_n)


# Both fully significant (both 2)
# L1
# for rho 0.5, beta 1: 100
df_comps2 %>% 
  filter(sig_beta > 1 & sig_pcor  > 1) %>% 
  filter(fit_a != fit_b) %>% 
  left_join(n_obs, by = join_by(fit_a == subj_id)) %>% 
  left_join(n_obs, by = join_by(fit_b == subj_id), suffix = c("_a", "_b")) %>% 
  mutate(diff_n = abs(n_a - n_b)) %>% 
  arrange(diff_n) 

# Both significant according to or rule
# L1
# for rho 0.5, beta 1: 442
df_comps %>% 
  filter(sig_beta > 0 & sig_pcor  > 0) %>% 
  filter(fit_a != fit_b) %>% 
  left_join(n_obs, by = join_by(fit_a == subj_id)) %>% 
  left_join(n_obs, by = join_by(fit_b == subj_id), suffix = c("_a", "_b")) %>% 
  mutate(diff_n = abs(n_a - n_b)) %>% 
  arrange(diff_n) 



# Only beta significant
# L1
# for rho 0.5, beta 1: 248
df_comps %>% 
  filter(sig_beta > 0 & sig_pcor == 0) %>% 
  filter(fit_a != fit_b) %>% 
  left_join(n_obs, by = join_by(fit_a == subj_id)) %>% 
  left_join(n_obs, by = join_by(fit_b == subj_id), suffix = c("_a", "_b")) %>% 
  mutate(diff_n = abs(n_a - n_b)) %>% 
  arrange(diff_n)

# Only pcor significant
# L1
# for rho 0.5, beta 1: 566
df_comps %>% 
  filter(sig_beta == 0 & sig_pcor > 0) %>% 
  filter(fit_a != fit_b) %>% 
  left_join(n_obs, by = join_by(fit_a == subj_id)) %>% 
  left_join(n_obs, by = join_by(fit_b == subj_id), suffix = c("_a", "_b")) %>% 
  mutate(diff_n = abs(n_a - n_b)) %>% 
  arrange(diff_n)


```



Visualize distributions of empirical differences
```{r}
df_comps %>% 
  filter(fit_a != fit_b) %>% 
  pivot_longer(cols = c("emp_beta", "emp_pcor"), names_to = "mat") %>% 
  ggplot(aes(x = value, fill = as.factor(mat))) +
  geom_density()
```



# Example: No evidence for differences
8 and 29
```{r}
# Look for examples with no evidence for diffs
df_comps %>% 
  filter(priors == 2) %>% 
  filter(fit_a != fit_b) %>% 
  filter(comp == "frob") %>% 
  mutate(across(c(contains("sig"), contains("emp")),
                ~as.numeric(.))) %>% 
  filter(sig_beta == 0 & sig_pcor == 0) %>% View()

# find number of observations
n_obs <- do.call(rbind, lapply(l_data, function(x) nrow(x)))

# Choose pair with similar n 
# 18 and 12
fit_12 <- l_fits[[2]][[12]]
fit_18 <- l_fits[[2]][[18]]

# Compare
res_emp_comp <- compare_var(fit_12, fit_18, return_all = TRUE)


# Plot automatically
plot.compare_var(res_emp_comp, name_a = "ID 12", name_b = "ID 18")


```

Create network plots below each other, then plot the comparison.
Do this manually to have more control beyond the functions.
```{r}
# First, all the networks

svg(filename = "figures/empirical_temp_networks.svg")
par(mfrow=c(1,2))
p12_temp <- qgraph::qgraph(fit_12$beta_mu, theme = "colorblind", layout = "circle",
                           color = ggokabeito::palette_okabe_ito()[1])

p18_temp <- qgraph::qgraph(fit_18$beta_mu, theme = "colorblind", layout = "circle",
                           color = ggokabeito::palette_okabe_ito()[2])

dev.off()

svg(filename = "figures/empirical_cont_networks.svg")
par(mfrow=c(1,2))

p12_cont <- qgraph::qgraph(fit_12$pcor_mu, theme = "colorblind", layout = "circle",
                           color = ggokabeito::palette_okabe_ito()[1])
p18_cont <- qgraph::qgraph(fit_18$pcor_mu, theme = "colorblind", layout = "circle",
                           color = ggokabeito::palette_okabe_ito()[2])
dev.off()


# Plot individually
svg(filename = "figures/empirical_network_12t.svg")

p12_temp <- qgraph::qgraph(fit_12$beta_mu, theme = "colorblind", layout = "circle",
                           color = ggokabeito::palette_okabe_ito()[1])
dev.off()

svg(filename = "figures/empirical_network_18t.svg")
p18_temp <- qgraph::qgraph(fit_18$beta_mu, theme = "colorblind", layout = "circle",
                           color = ggokabeito::palette_okabe_ito()[2])
dev.off()

svg(filename = "figures/empirical_network_12c.svg")
p12_cont <- qgraph::qgraph(fit_12$pcor_mu, theme = "colorblind", layout = "circle",
                           color = ggokabeito::palette_okabe_ito()[1])
dev.off()

svg(filename = "figures/empirical_network_18c.svg")

p18_cont <- qgraph::qgraph(fit_18$pcor_mu, theme = "colorblind", layout = "circle",
                           color = ggokabeito::palette_okabe_ito()[2])
dev.off()



# Then, comparison for temporal and contemporaneous
plt_beta <- 
  res_emp_comp$res_beta %>%
    mutate(mod = case_match(mod,
                            "mod_a" ~ "ID 12",
                            "mod_b" ~ "ID 18")) %>% 
    ggplot(aes(x = null, fill = mod))+
    geom_density(alpha = .7)+
    theme_classic()+
    ggokabeito::scale_fill_okabe_ito()+
    geom_vline(aes(xintercept = res_emp_comp$emp_beta), 
               col = "black", lty = 1, linewidth = .75)+
    scale_y_continuous(expand = c(0,0))+
    theme_compare()+
    labs(y = "",
         x = "Norm Value",
         fill = "",
         title = "Test")+
    theme(axis.ticks.y = element_blank(),
          axis.text.y = element_blank(),
          legend.position = "right",
          legend.key.height= unit(0.5, 'cm'),
          legend.key.width= unit(1, 'cm'),
          plot.title = element_text(face = "plain", family = "news", size = 9, hjust = 0))
plt_beta

plt_pcor <- 
  res_emp_comp$res_pcor %>%
    mutate(mod = case_match(mod,
                            "mod_a" ~ "ID 12",
                            "mod_b" ~ "ID 18")) %>% 
    ggplot(aes(x = null, fill = mod))+
    geom_density(alpha = .7)+
    theme_classic()+
    ggokabeito::scale_fill_okabe_ito()+
    geom_vline(aes(xintercept = res_emp_comp$emp_pcor), 
               col = "black", lty = 1, linewidth = .75)+
    scale_y_continuous(expand = c(0,0))+
    labs(y = "",
         x = "Norm Value")+
    theme_compare()+
    theme(axis.ticks.y = element_blank(),
          axis.text.y = element_blank(),
          legend.position = "right")

leg <- get_legend(plt_beta)
  
# Plot
plt_tmp <- cowplot::plot_grid(plt_beta + theme(legend.position = "none"),
                              plt_pcor + theme(legend.position = "none"))

# Add legend
# plt <- plot_grid(plt_tmp, leg, rel_widths = c(3, .4))
plt <- plt_tmp 
plt

plt_pcor

# Figure had to be created manually because qgraph cannot be used by cowplot...
svg_12t <- magick::image_read_svg(here::here("figures/empirical_network_12t.svg"))
svg_12c <- magick::image_read_svg(here::here("figures/empirical_network_12c.svg"))
svg_18t <- magick::image_read_svg(here::here("figures/empirical_network_18t.svg"))
svg_18c <- magick::image_read_svg(here::here("figures/empirical_network_18c.svg"))


plt_12t <- ggdraw() + 
  draw_image(svg_12t, scale = 1) +
  draw_label("ID 12", x = 0.1, y = 0.85, fontfamily = "news", size = 9)
plt_12t
plt_12c <- ggdraw() + 
  draw_image(svg_12c, scale = 1)
plt_18t <- ggdraw() + 
  draw_image(svg_18t, scale = 1) + 
  draw_label("ID 18", x = 0.1, y = 0.85, fontfamily = "news", size = 9)
plt_18c <- ggdraw() + draw_image(svg_18c, scale = 1)

row_12 <- plot_grid(plt_12t, plt_12c, 
                    labels = c("Temporal", "Contemporaneous"),
                    label_size = 14,
                    label_fontfamily = "news",
                    label_x = c(-0.1, -0.2))
row_18 <- plot_grid(plt_18t, plt_18c)


full_plot <- plot_grid(
  row_12,
  row_18,
  plt,
  nrow = 3,
  rel_heights = c(1,1,0.7)
)

ggsave("full_plot_empirical_example.svg", full_plot, device = "svg", height = 8.5, width = 6, 
       path = here::here("figures/"))

```








## Posterior matrix
Idea: Plot posterior distributions, color based on if the parameter is positive or not. 
```{r}
fit <- l_fits_c[[1]][[1]]

plot_example_posterior <- posterior_plot(fit, mat = beta)

plot_example_posterior+geom_text()

ggsave("plot_example_posterior_beta.svg", plot_example_posterior, 
       device = "svg", path = here::here("figures/"), width = 20, height = 12, 
       units = "cm")

```


# Random investigation: Handling Missings/Nights

```{r}
data_1 <- data %>% 
  filter(subj_id == 1) %>% 
  select(energetic:accepted)

fit_1 <- BGGM::var_estimate(as.data.frame(data_1))

fit_1$beta_mu

# Now delete missing data
data_1_nomiss <- data_1 %>% 
  filter(!is.na(accepted)) %>% 
  as.data.frame()

fit_1_nomiss <- BGGM::var_estimate(as.data.frame(data_1_nomiss))

fit_1_nomiss$beta_mu
fit_1_nomiss$pcor_mu

```
The results are identical. 
Now try something else: inserting missing data between rows
```{r}
#Number of empty rows to insert
N = 1
df1 <- data_1_nomiss
rownames(df1) <- NULL

#Every N rows after which empty rows should be inserted
after_rows = 4

do.call(rbind, lapply(split(df1, ceiling(1:NROW(df1)/after_rows)),
                      function(a) rbind(a, replace(a[1:N,], TRUE, NA))))

rownames(df1) <- NULL

```

Fit model again
```{r}
fit_1_insmiss <- BGGM::var_estimate(df1)
fit_1_insmiss$beta_mu
fit_1_insmiss$pcor_mu

```






