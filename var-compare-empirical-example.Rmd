---
title: "var-compare-empirical-example"
author: "Bj√∂rn Siepe"
date: "2023-01-30"
output: html_document
---

We use data from Fisher et al. (2017), downloaded from https://github.com/jmbh/EmotionTimeSeries/blob/master/DataClean/Fisher2017/data_Fisher2017.RDS. 

# Preparation and Data Cleaning
```{r}
library(BGGM)
library(qgraph)
library(tidyverse)
library(foreach)
library(doParallel)
library(parallel)
source("aux_funs.R")

data <- readRDS("data_Fisher2017.RDS")


# Number of observations per model
n_obs <- data %>% 
  count(subj_id)

# Use 6 items
short_data <- data %>% 
  # dplyr::filter(subj_id %in% rel_ids) %>% 
  dplyr::select(subj_id, content, worried, anhedonia, positive, fatigue, accepted) %>% 
  group_by(subj_id) %>% 
  mutate(tp = row_number()) %>% 
  ungroup() %>% 
  split(.$subj_id)


short_data <- data %>% 
  # dplyr::filter(subj_id %in% rel_ids) %>% 
  dplyr::select(subj_id, reassure, down, anhedonia, enthusiastic, hopeless, accepted) %>% 
  group_by(subj_id) %>% 
  mutate(tp = row_number()) %>% 
  ungroup() %>% 
  split(.$subj_id)



```


## Detrending

TODO: Double check if this worked
```{r}
# Relevant variables
rel_vars <- c("content", "worried", "anhedonia", "positive", "fatigue", "accepted")
rel_vars <- c("reassure", "down", "anhedonia", "enthusiastic", "hopeless", "accepted")

data_detrend <- lapply(short_data, function(x){
  for (v in 1:length(rel_vars)){
  # Respektive Variable auf die Zeit regressieren
  lm_form <- as.formula(paste0(rel_vars[v], "~ tp"))
  # lineares Modell rechnen
  lm_res <- summary(lm(lm_form, data = x))
  # wenn der Zeittrend signifikant ist, detrenden wir mit den Residuen
  # [,4] greift auf die Spalte der p-Werte zu
  # [2] auf den p-Wert des Regressionsgewichts des Datums
  if(lm_res$coefficients[,4][2] < 0.05){
    print(paste0("Detrende Variable: ", rel_vars[v]))
    x[!is.na(x[rel_vars[v]]),rel_vars[v]] <- residuals(lm_res)
  }
  
}
return(x)
})





```



# Fitting
Fit all models
```{r}
rho_prior = 0.25
beta_prior = .5
iterations = 50000
seed = 2022

df_prior <- data.frame(
  rho_prior = c(0.1, 0.25, 0.5),
  beta_prior = c(0.25, 0.5, 1)
)

# Investigate prior sensitivity with fixed beta prior
# df_prior_2 <- data.frame(
#   rho_prior = c(0.2, 0.3, 0.4),
#   beta_prior = rep(.6, 3)
# )
# or fixed rho prior
df_prior_2 <- data.frame(
  rho_prior = rep(0.4,3),
  beta_prior = c(.5, .7, .9)
)


l_fits <- list()

# Delete id and tp column for fitting
data_detrend <- lapply(data_detrend, function(x){
  x <- x[,-c(1,8)]
  x
})

cl = makeCluster(3)
registerDoParallel(cl)
l_fits <- foreach(i = 1:nrow(df_prior), .packages = "BGGM") %dopar% {
  r_p <- df_prior[i,"rho_prior"]
  b_p <- df_prior[i, "beta_prior"]
  
  fit <- lapply(data_detrend, function(x){
  tryCatch(BGGM::var_estimate(Y = x,      # delete id column and tp column for fitting
                     rho_sd = r_p,
                     beta_sd = b_p,
                     iter = 50000,
                     seed = 2022), error = function(e) NA)
  # BGGM::var_estimate(Y = as.data.frame(x),
  #                    rho_sd = r_p,
  #                    beta_sd = b_p,
  #                    iter = 50000,
  #                    seed = 2022)
    
    
})
  # if(is.list(fit)){
  #   lapply(fit, function(x){
  #     x$fit$Sigma <- NULL
  #     x$fit$fisher_z <- NULL
  #   })

  # }
  return(fit)
}

stopCluster(cl)  
  
l_fits_c <- list()
for(i in 1:3){
  l_fits_c[[i]] <- lapply(l_fits[[i]], function(x){
  x$fit$Sigma <- NULL
  x$fit$fisher_z <- NULL
  x
})
  
}

# saveRDS(l_fits_c, here::here("data/empirical_example/l_fits_empirical_ex.RDS"))



```
## Investigate distribution of partial correlations
```{r}
pcor_list <- lapply(l_fits_c[[3]], function(x) return(x$pcor_mu))
beta_list <- lapply(l_fits_c[[3]], function(x) return(x$beta_mu))


pcors <- do.call(c, pcor_list)
betas <- do.call(c, beta_list)

plot(density(pcors))
plot(density(betas))

plot(density(rnorm(1000, mean = 0, sd = .6)))
plot_prior(.7)

```




## Convergence 

```{r}
# Trace Plots
BGGM::convergence(l_fits[[1]], type = "trace",  param = "reassure--accepted")

# ACF
BGGM::convergence(l_fits[[1]], type = "acf", param = "down--enthusiastic")


# pdf("convergence_empirical.pdf")
# for(i in 1:10){
#   for(n in param_names){
# 
#   print(BGGM::convergence(l_fits[[1]], type = "acf", param = n))
#   
# }
# }
# dev.off()

# Try to calculate effective sample size
bet <- l_fits[[1]]$fit$beta[,,51:iterations+50]

# Convert to matrix where each col is a variable
m_bet <- t(matrix(bet, 6*6, iterations))
mcmc_bet <- as.mcmc(m_bet)
coda::effectiveSize(mcmc_bet)

# Same for pcor
pcor <- l_fits[[1]]$fit$pcors[,,51:iterations+50]
m_pcor <- t(matrix(pcor, 6*6, iterations))
mcmc_pcor <- as.mcmc(m_pcor)
ces <- coda::effectiveSize(mcmc_pcor)


# Get ESS for all samples
l_ess <- lapply(l_fits, var_ess)

```


## Plot all networks

```{r}
plot_net <- function(fit_obj){
  par(mfrow=c(1,2))
  qgraph::qgraph(fit_obj$beta_mu, title = "Temporal", theme = "colorblind", layout = "circle")
  qgraph::qgraph(fit_obj$pcor_mu, title = "Contemporaneous", theme = "colorblind", layout = "circle")
}


```



 
Test across all participants and across different priors. 
```{r}
l_fits <- l_fits_c
fit_grid <- expand.grid(fit_a = as.numeric(names(l_fits[[1]])), 
                        fit_b = as.numeric(names(l_fits[[1]])), 
                        priors = seq(nrow(df_prior), 1), 
                        comp = c("l1", "frob", "maxdiff"))
l_comps <- list()

cl = makeCluster(14)
registerDoParallel(cl)
clusterExport(cl, c("compare_var", "post_distance_within"))
# Rewrite in parallel





l_fits_1 <- l_fits[[1]]
fit_grid_1 <- fit_grid %>% 
  filter(priors == 1)
l_comps_1 <- foreach(i = 1:nrow(fit_grid_1)) %dopar% {
  
  # Extract information
  a <- fit_grid_1$fit_a[i]
  b <- fit_grid_1$fit_b[i]
  p_ind <- fit_grid_1$priors[i]
  compa <-  as.character(fit_grid_1$comp[i])


  
  if(is.list(l_fits_1[[a]]) & is.list(l_fits_1[[b]])){

  comp_res  <- compare_var(l_fits_1[[a]],l_fits_1[[b]], comp = compa)

  return(comp_res)
  }
  else
    return(NA)
  
  
    
  
  
}



l_fits_2 <- l_fits_c[[2]]
fit_grid_2 <- fit_grid %>% 
  filter(priors == 2)
l_comps_2 <- foreach(i = 1:nrow(fit_grid_2)) %dopar% {
  
  # Extract information
  a <- fit_grid_2$fit_a[i]
  b <- fit_grid_2$fit_b[i]
  p_ind <- fit_grid_2$priors[i]
  compa <-  as.character(fit_grid_2$comp[i])

  
  # Compare
  # if(is.list(l_fits[[p_ind]][[a]]) & is.list(l_fits[[p_ind]][[b]])){
  # # comp_res  <- tryCatch({compare_var(l_fits_2[[a]],l_fits_2[[b]], pred = FALSE, comp = comp)},
  #                       # error = function(e) NULL)
  # comp_res  <- compare_var(l_fits[[p_ind]][[a]],l_fits[[p_ind]][[b]], comp = comp)
  
  
  if(is.list(l_fits_2[[a]]) & is.list(l_fits_2[[b]])){
  # comp_res  <- tryCatch({compare_var(l_fits_2[[a]],l_fits_2[[b]], pred = FALSE, comp = comp)},
                        # error = function(e) NULL)
  comp_res  <- compare_var(l_fits_2[[a]],l_fits_2[[b]], comp = compa)

  return(comp_res)
  }
  else
    return(NA)
  
  
    
  
  
}

rm(l_fits_2)



l_fits_3 <- l_fits_c[[3]]
fit_grid_3 <- fit_grid %>% 
  filter(priors == 3)
l_comps_3 <- foreach(i = 1:nrow(fit_grid_3)) %dopar% {
  
  # Extract information
  a <- fit_grid_3$fit_a[i]
  b <- fit_grid_3$fit_b[i]
  p_ind <- fit_grid_3$priors[i]
  compa <-  as.character(fit_grid_3$comp[i])

  
  # Compare
  # if(is.list(l_fits[[p_ind]][[a]]) & is.list(l_fits[[p_ind]][[b]])){
  # # comp_res  <- tryCatch({compare_var(l_fits_3[[a]],l_fits_3[[b]], pred = FALSE, comp = comp)},
  #                       # error = function(e) NULL)
  # comp_res  <- compare_var(l_fits[[p_ind]][[a]],l_fits[[p_ind]][[b]], comp = comp)
  
  
  if(is.list(l_fits_3[[a]]) & is.list(l_fits_3[[b]])){
  # comp_res  <- tryCatch({compare_var(l_fits_3[[a]],l_fits_3[[b]], pred = FALSE, comp = comp)},
                        # error = function(e) NULL)
  comp_res  <- compare_var(l_fits_3[[a]],l_fits_3[[b]], comp = compa)

  return(comp_res)
  }
  else
    return(NA)
  
  
    
  
  
}
stopCluster(cl)
rm(l_fits_3)

df_comps2_1 <- do.call(rbind, l_comps_1)
df_comps2_2 <- do.call(rbind, l_comps_2)
df_comps2_3 <- do.call(rbind, l_comps_3)

# Add prior information
df_comps2_1c <- df_comps2_1 %>% 
  cbind(fit_grid_1)
df_comps2_2c <- df_comps2_2 %>% 
  cbind(fit_grid_2)
df_comps2_3c <- df_comps2_3 %>% 
  cbind(fit_grid_3)



df_compsc2 <- as.data.frame(rbind(df_comps_1c, df_comps_2c, df_comps_3c))

# delete unnecessary columns
df_comps2 <- df_compsc2 %>% 
  dplyr::select(-c(larger_beta, larger_pcor))





# saveRDS(df_comps, here::here("output/empirical-example/df_comps.RDS"))
df_comps <- readRDS(here::here("output/empirical-example/df_comps.RDS"))




```

### Prior Sensitivity Plot
```{r}
# Calculate amount of possible combinations per condition
# 1560
df_comps2 %>% 
  filter(fit_a != fit_b) %>% 
  group_by(priors, comp) %>% 
  count()


prior_names <- c(
  "1" = "Rho: 0.1, Beta: 0.25",
  "2" = "Rho: 0.25, Beta: 0.5",
  "3" = "Rho: 0.5, Beta: 1"
)

# prior_names2 <- c(
#   "1" = "Rho: 0.2, Beta: 0.6",
#   "2" = "Rho: 0.3, Beta: 0.6",
#   "3" = "Rho: 0.4, Beta: 0.6"
# )
prior_names2 <- c(
  "1" = "Rho: 0.4, Beta: 0.5",
  "2" = "Rho: 0.4, Beta: 0.7",
  "3" = "Rho: 0.4, Beta: 0.9"
)


emp_ex_prior_sensitivity2 <- df_comps2 %>% 
  filter(fit_a != fit_b) %>% 
  mutate(across(c(contains("beta"), contains("pcor")),
                ~as.numeric(.))) %>% 
  group_by(priors, comp) %>% 
  summarize(sum_sig_beta = sum(ifelse(sig_beta >0, 1, 0)),
            sum_sig_pcor = sum(ifelse(sig_pcor >0, 1, 0)),
            mean_emp_beta = mean(emp_beta, na.rm =  TRUE)) %>% 
  mutate(prop_sig_beta = sum_sig_beta / 1560,
         prop_sig_pcor = sum_sig_pcor / 1560) %>% 
  pivot_longer(cols = c("sum_sig_beta", "sum_sig_pcor"), names_to = "mat_sum", values_to = "sum") %>% 
  pivot_longer(cols = c("prop_sig_beta", "prop_sig_pcor"), names_to = "mat_prop", values_to = "prop") %>% 
  ggplot(aes(x = comp, y = prop, group = mat_prop, fill = mat_prop))+
  geom_bar(position = "dodge", stat = "identity")+
  facet_wrap(priors~., labeller = as_labeller(prior_names2))+
  theme_minimal()+
  ggokabeito::scale_fill_okabe_ito(labels = c("Beta", "PCor"))+
  scale_y_continuous(limits = c(0,1),labels = function(x) paste0(x*100, "%"))+
  labs(x = "Norm",
       y = "Proportion Significant",
       title = "Prior Sensitivity Empirical Example",
       fill = "Matrix")

ggsave("emp_ex_prior_sensitivity2.svg", emp_ex_prior_sensitivity2, device = "svg", path = here::here("figures/"), width = 7, height = 5)

```




### Find significant and nonsignificant
Extract interesting cases
```{r}
# Find interesting cases with similar amount of observations
# nonsignificant 
# L1
# for rho 0.5, beta 1: 304
df_comps %>% 
  filter(sig_beta == 0 & sig_pcor  == 0) %>% 
  filter(fit_a != fit_b) %>% 
  left_join(n_obs, by = join_by(fit_a == subj_id)) %>% 
  left_join(n_obs, by = join_by(fit_b == subj_id), suffix = c("_a", "_b")) %>% 
  mutate(diff_n = abs(n_a - n_b)) %>% 
  arrange(diff_n)


# Both fully significant (both 2)
# L1
# for rho 0.5, beta 1: 100
df_comps %>% 
  filter(sig_beta > 1 & sig_pcor  > 1) %>% 
  filter(fit_a != fit_b) %>% 
  left_join(n_obs, by = join_by(fit_a == subj_id)) %>% 
  left_join(n_obs, by = join_by(fit_b == subj_id), suffix = c("_a", "_b")) %>% 
  mutate(diff_n = abs(n_a - n_b)) %>% 
  arrange(diff_n) 

# Both significant according to or rule
# L1
# for rho 0.5, beta 1: 442
df_comps %>% 
  filter(sig_beta > 0 & sig_pcor  > 0) %>% 
  filter(fit_a != fit_b) %>% 
  left_join(n_obs, by = join_by(fit_a == subj_id)) %>% 
  left_join(n_obs, by = join_by(fit_b == subj_id), suffix = c("_a", "_b")) %>% 
  mutate(diff_n = abs(n_a - n_b)) %>% 
  arrange(diff_n) 



# Only beta significant
# L1
# for rho 0.5, beta 1: 248
df_comps %>% 
  filter(sig_beta > 0 & sig_pcor == 0) %>% 
  filter(fit_a != fit_b) %>% 
  left_join(n_obs, by = join_by(fit_a == subj_id)) %>% 
  left_join(n_obs, by = join_by(fit_b == subj_id), suffix = c("_a", "_b")) %>% 
  mutate(diff_n = abs(n_a - n_b)) %>% 
  arrange(diff_n)

# Only pcor significant
# L1
# for rho 0.5, beta 1: 566
df_comps %>% 
  filter(sig_beta == 0 & sig_pcor > 0) %>% 
  filter(fit_a != fit_b) %>% 
  left_join(n_obs, by = join_by(fit_a == subj_id)) %>% 
  left_join(n_obs, by = join_by(fit_b == subj_id), suffix = c("_a", "_b")) %>% 
  mutate(diff_n = abs(n_a - n_b)) %>% 
  arrange(diff_n)


```







Visualize distributions of empirical differences
```{r}
df_comps %>% 
  filter(fit_a != fit_b) %>% 
  pivot_longer(cols = c("emp_beta", "emp_pcor"), names_to = "mat") %>% 
  ggplot(aes(x = value, fill = as.factor(mat))) +
  geom_density()
```




# Example A: Evidence for differences
27 and 3
```{r}
compare_var(l_fits[[8]], l_fits[[27]])
plot_net(l_fits[[8]])
plot_net(l_fits[[27]])


# Visualize differences
it_diff <- which(df_comps$fit_a == 8 & df_comps$fit_b == 27)
plot.compare_var(l_comps[[it_diff]])

```






# Example B: No evidence for differences
8 and 29
```{r}
compare_var(l_fits[[8]], l_fits[[29]])

plot_net(l_fits[[8]])
plot_net(l_fits[[29]])


# Visualize Differences
# Find iteration number
it_nodiff <- which(df_comps$fit_a == 8 & df_comps$fit_b == 29)

plot.compare_var(l_comps[[it_nodiff]])


```




# Plot Distances visualized as Network

Extract distances: 
```{r}
df_emp <- data.frame(emp_beta = rep(NA, 1600),
                     emp_pcor = rep(NA, 1600))
for(n in 1:length(l_comps)){
  df_emp[n,1] <- l_comps[[n]]$emp_beta
  df_emp[n,2] <- l_comps[[n]]$emp_pcor
}
# Combine with model information
df_emp <- cbind(df_emp, fit_grid)

# Filter irrelevant comparisons
df_emp <- df_emp %>% 
  filter(fit_a != fit_b)


# Correct data structure: 40 by 40 matrix
dist_beta_emp <- df_emp %>% 
  dplyr::select(emp_beta, fit_a, fit_b) %>% 
  reshape2::acast(., fit_a ~ fit_b, value.var = "emp_beta")


dist_pcor_emp <- df_emp %>% 
  dplyr::select(emp_pcor, fit_a, fit_b) %>% 
  reshape2::acast(., fit_a ~ fit_b, value.var = "emp_pcor")


```


Now visualize with qgraph
```{r}
# The shorter the distance, the larger the connection should be!
# Convert distance to similarity matrix
simil_beta_emp <- 1/mat_beta_emp
simil_pcor_emp <- 1/mat_pcor_emp

svg("emp_example_similarity_net.svg", width = 15, height = 12)
par(mfrow = c(1,2))
qgraph(simil_beta_emp, layout = "spring", title = "Beta")
qgraph(simil_pcor_emp, layout = "spring", title = "PCOR")
dev.off()


```


Also visualize with heatmap
```{r}
# Levels
id_lvls <- as.character(1:40)

# Get min and max similarities
min(simil_beta_emp, na.rm = TRUE)
min(simil_pcor_emp, na.rm = TRUE)
max(simil_beta_emp, na.rm = TRUE)
max(simil_pcor_emp, na.rm = TRUE)



p1 <- simil_beta_emp %>% 
  as.data.frame() %>% 
  rownames_to_column("id") %>% 
  pivot_longer(-c(id), names_to = "id2", values_to = "sim") %>% 
  mutate(id = fct_relevel(id, id_lvls),
         id2 = fct_relevel(id2, id_lvls)) %>% 
  ggplot(aes(x = id, y = id2, fill = sim))+
  geom_raster()+
  scale_fill_viridis_c(limits = c(0.1, 0.6))+
  labs(x = "ID",
       y = "ID",
       title = "Similarity Matrix Beta Weights",
       caption = "Frobenius Norm",
       fill = "Similarity")


p2 <- simil_pcor_emp %>% 
  as.data.frame() %>% 
  rownames_to_column("id") %>% 
  pivot_longer(-c(id), names_to = "id2", values_to = "sim") %>% 
  mutate(id = fct_relevel(id, id_lvls),
         id2 = fct_relevel(id2, id_lvls)) %>% 
  ggplot(aes(x = id, y = id2, fill = sim))+
  geom_raster()+
  scale_fill_viridis_c(limits = c(0.1, 0.6))+
  labs(x = "ID",
       y = "ID",
       title = "Similarity Matrix pcor Weights",
       caption = "Frobenius Norm",
       fill = "Similarity")

p <- cowplot::plot_grid(p1, p2)
ggsave("similarity_matrix_empirical_example.svg", p, device = "svg", path = here("figures/"), width = 15)



```




